# import streamlit as st
# import gensim
# import numpy as np
# import pandas as pd
# from sklearn.metrics.pairwise import cosine_similarity
# import umap
# import plotly.express as px
# import os

# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
# @st.cache_resource
# def load_model(model_path):
#     try:
#         model = None
#         model_type = None
        
#         # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
#         if 'fasttext' in model_path.lower():
#             model = gensim.models.FastText.load(model_path)
#             model_type = 'fasttext'
#         elif 'word2vec' in model_path.lower():
#             model = gensim.models.Word2Vec.load(model_path)
#             model_type = 'word2vec'
#         elif 'doc2vec' in model_path.lower() or 'd2v' in model_path.lower():
#             model = gensim.models.Doc2Vec.load(model_path)
#             model_type = 'doc2vec'
#         else:
#             st.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—É—Ç–∏: {model_path}")
#             return None, None

#         # –î–ª—è Doc2Vec –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º –≤—ã–±–æ—Ä –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Å–ª–æ–≤ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
#         if model_type == 'doc2vec':
#             if hasattr(model, 'wv') and len(model.wv) > 0:
#                 return model.wv, 'doc2vec_words'
#             elif hasattr(model, 'dv') and len(model.dv) > 0:
#                 return model.dv, 'doc2vec_docs'
#             else:
#                 st.error("Doc2Vec –º–æ–¥–µ–ª—å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤, –Ω–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
#                 return None, None
#         elif model and hasattr(model, 'wv'):
#             return model.wv, model_type
#         else:
#             st.error(f"–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏ –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ '.wv'. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–∏–ø –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.")
#             return None, None

#     except FileNotFoundError:
#         st.error(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {model_path}")
#         return None, None
#     except Exception as e:
#         st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_path}: {e}")
#         st.exception(e)
#         return None, None

# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è —Ç–æ–∫–µ–Ω–∞ –≤ –º–æ–¥–µ–ª–∏
# def token_in_model(model, token, model_type):
#     if model_type == 'doc2vec_docs':
#         # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–≥–∞
#         return token in model
#     else:
#         # –î–ª—è —Å–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
#         return token in model

# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞
# def get_vector(model, token, model_type):
#     if model_type == 'doc2vec_docs':
#         # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
#         return model[token]
#     else:
#         # –î–ª—è —Å–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥
#         return model[token]

# # --- Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ---
# st.title("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤")

# st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")

# # –ü–æ–∏—Å–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ü–µ–ª–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
# path_to_models = os.path.join("–¢–µ–∫—Å—Ç –ê–Ω–∞–ª–∏–∑", "models")
# model_files = [f for f in os.listdir(path_to_models) if f.endswith('.model')]
# selected_model_path = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:", model_files)

# model_wv = None
# model_type = None

# if selected_model_path:
#     full_model_path = os.path.join(path_to_models, selected_model_path)
#     model_wv, model_type = load_model(full_model_path)

# if model_wv is None:
#     st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è.")
# else:
#     st.success(f"–ú–æ–¥–µ–ª—å '{selected_model_path}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –¢–∏–ø: {model_type}. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(model_wv)}")

#     # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è Doc2Vec –º–æ–¥–µ–ª–µ–π —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
#     if model_type == 'doc2vec_docs':
#         st.info("‚ö†Ô∏è –ó–∞–≥—Ä—É–∂–µ–Ω–∞ Doc2Vec –º–æ–¥–µ–ª—å —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –†–∞–±–æ—Ç–∞–π—Ç–µ —Å —Ç–µ–≥–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤.")

#     # --- 1. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ ---
#     st.header("1. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞")
    
#     if model_type == 'doc2vec_docs':
#         st.write("–í–≤–µ–¥–∏—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–µ–≥–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'doc1 - doc2 + doc3'")
#     else:
#         st.write("–í–≤–µ–¥–∏—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ '—Å–ª–æ–≤–æ1 - —Å–ª–æ–≤–æ2 + —Å–ª–æ–≤–æ3' (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏).")

#     default_example = "–ø—É—Ç–∏–Ω - –º—É–∂—á–∏–Ω + –∂–µ–Ω—â–∏–Ω" if model_type != 'doc2vec_docs' else "DOC_1 - DOC_2 + DOC_3"
#     arithmetic_input = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ:", default_example)

#     if st.button("–í—ã—á–∏—Å–ª–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—É—é –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫—É"):
#         if model_wv:
#             try:
#                 # –ü–∞—Ä—Å–∏–Ω–≥ –≤—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
#                 parts = arithmetic_input.split()
#                 positive = []
#                 negative = []
#                 current_op = '+'
#                 valid_input = True

#                 for part in parts:
#                     if part == '+':
#                         current_op = '+'
#                     elif part == '-':
#                         current_op = '-'
#                     elif token_in_model(model_wv, part, model_type):
#                         if current_op == '+':
#                             positive.append(part)
#                         else:
#                             negative.append(part)
#                     else:
#                         st.warning(f"–¢–æ–∫–µ–Ω '{part}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–≤–µ—Ä–µ–Ω.")
#                         valid_input = False
#                         break

#                 if valid_input and (positive or negative):
#                     st.write(f"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ: {' + '.join(positive)} - {' - '.join(negative)}")
#                     try:
#                         # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
#                         if positive:
#                             result_vector = get_vector(model_wv, positive[0], model_type).copy()
#                         else:
#                             result_vector = np.zeros(model_wv.vector_size)
                        
#                         for token in positive[1:]:
#                             result_vector += get_vector(model_wv, token, model_type).copy()
#                         for token in negative:
#                             result_vector -= get_vector(model_wv, token, model_type).copy()

#                         st.write("–ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞:")
                        
#                         # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
#                         if model_type == 'doc2vec_docs':
#                             # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π –ø–æ–¥—Ö–æ–¥
#                             similarities = []
#                             for doc_tag in list(model_wv.key_to_index.keys())[:100]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ–∏—Å–∫
#                                 doc_vector = get_vector(model_wv, doc_tag, model_type)
#                                 similarity = cosine_similarity([result_vector], [doc_vector])[0][0]
#                                 similarities.append((doc_tag, similarity))
                            
#                             # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
#                             similarities.sort(key=lambda x: x[1], reverse=True)
#                             for token, similarity in similarities[:10]:
#                                 st.write(f"- {token} (–°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f})")
#                         else:
#                             # –î–ª—è —Å–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
#                             most_similar_results = model_wv.most_similar(positive=positive, negative=negative, topn=10)
#                             for word, similarity in most_similar_results:
#                                 st.write(f"- {word} (–°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f})")

#                     except Exception as e:
#                         st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –∏–ª–∏ –ø–æ–∏—Å–∫–µ —Å–æ—Å–µ–¥–µ–π: {e}")

#                 elif valid_input:
#                     st.warning("–í–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏.")

#             except Exception as e:
#                 st.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–≤–æ–¥–∞: {e}")
#         else:
#             st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

#     # --- 2. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º ---
#     st.header("2. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º")

#     label_1 = "–¢–µ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ 1:" if model_type == 'doc2vec_docs' else "–°–ª–æ–≤–æ 1:"
#     label_2 = "–¢–µ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ 2:" if model_type == 'doc2vec_docs' else "–°–ª–æ–≤–æ 2:"
    
#     default_1 = "DOC_1" if model_type == 'doc2vec_docs' else "–ø—É—Ç–∏–Ω"
#     default_2 = "DOC_2" if model_type == 'doc2vec_docs' else "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç"
    
#     token1_sim = st.text_input(label_1, default_1)
#     token2_sim = st.text_input(label_2, default_2)

#     if st.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"):
#         if model_wv:
#             if (token_in_model(model_wv, token1_sim, model_type) and 
#                 token_in_model(model_wv, token2_sim, model_type)):
#                 try:
#                     if model_type == 'doc2vec_docs':
#                         # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –≤—Ä—É—á–Ω—É—é
#                         vec1 = get_vector(model_wv, token1_sim, model_type)
#                         vec2 = get_vector(model_wv, token2_sim, model_type)
#                         similarity = cosine_similarity([vec1], [vec2])[0][0]
#                     else:
#                         # –î–ª—è —Å–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
#                         similarity = model_wv.similarity(token1_sim, token2_sim)
                    
#                     entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
#                     st.write(f"–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É '{token1_sim}' –∏ '{token2_sim}': {similarity:.4f}")
#                 except Exception as e:
#                     st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
#             else:
#                 oov_tokens = [t for t in [token1_sim, token2_sim] if not token_in_model(model_wv, t, model_type)]
#                 entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
#                 st.warning(f"–û–¥–∏–Ω –∏–ª–∏ –æ–±–∞ {entity_type} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –º–æ–¥–µ–ª–∏: {', '.join(oov_tokens)}")
#         else:
#             st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

#     # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã (–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–µ–π, UMAP) —Ç–∞–∫–∂–µ –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è Doc2Vec
#     # –î–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–ª—é –∏—Ö –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –Ω–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∏—Ö —Ç–æ–∂–µ –Ω—É–∂–Ω–æ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å

#     # --- 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–µ–π ---
#     st.header("3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–µ–π")
    
#     if model_type == 'doc2vec_docs':
#         st.write("–í—ã–±–µ—Ä–∏—Ç–µ –¥–≤–∞ —Ç–µ–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Å–∏.")
#         pole1_label = "–ü–æ–ª—é—Å 1 (—Ç–µ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞):"
#         pole2_label = "–ü–æ–ª—é—Å 2 (—Ç–µ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞):"
#         default_pole1 = "DOC_1"
#         default_pole2 = "DOC_2"
#     else:
#         st.write("–í—ã–±–µ—Ä–∏—Ç–µ –¥–≤–∞ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Å–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥—Ä—É–≥–∏—Ö —Å–ª–æ–≤.")
#         pole1_label = "–ü–æ–ª—é—Å 1:"
#         pole2_label = "–ü–æ–ª—é—Å 2:"
#         default_pole1 = "–º—É–∂—á–∏–Ω"
#         default_pole2 = "–∂–µ–Ω—â–∏–Ω"

#     axis_token1 = st.text_input(pole1_label, default_pole1)
#     axis_token2 = st.text_input(pole2_label, default_pole2)
#     num_tokens_on_axis = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏:", 10, 100, 30)

#     if st.button("–í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Å—å"):
#         if model_wv:
#             if (token_in_model(model_wv, axis_token1, model_type) and 
#                 token_in_model(model_wv, axis_token2, model_type)):
#                 try:
#                     # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –æ—Å–∏
#                     axis_vector = (get_vector(model_wv, axis_token2, model_type) - 
#                                  get_vector(model_wv, axis_token1, model_type))

#                     # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏
#                     all_tokens = list(model_wv.key_to_index.keys())
#                     # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
#                     start_idx = min(50, len(all_tokens) - num_tokens_on_axis)
#                     tokens_to_project = all_tokens[start_idx:start_idx + num_tokens_on_axis]

#                     # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ–µ–∫—Ü–∏–∏
#                     projections = []
#                     token_labels = []
#                     for token in tokens_to_project:
#                         if token_in_model(model_wv, token, model_type):
#                             token_vector = get_vector(model_wv, token, model_type)
#                             # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä –æ—Å–∏
#                             norm_axis_vector = axis_vector / np.linalg.norm(axis_vector)
#                             projection_value = np.dot(token_vector, norm_axis_vector)
#                             projections.append(projection_value)
#                             token_labels.append(token)

#                     # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
#                     projection_df = pd.DataFrame({'Token': token_labels, 'Projection': projections})
#                     projection_df = projection_df.sort_values('Projection')

#                     # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
#                     entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
#                     fig = px.bar(projection_df, x='Projection', y='Token', orientation='h',
#                                  title=f'–ü—Ä–æ–µ–∫—Ü–∏—è {entity_type} –Ω–∞ –æ—Å—å "{axis_token1}" - "{axis_token2}"')
#                     st.plotly_chart(fig)

#                 except Exception as e:
#                     st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å–∏: {e}")
#             else:
#                 oov_tokens = [t for t in [axis_token1, axis_token2] if not token_in_model(model_wv, t, model_type)]
#                 entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
#                 st.warning(f"–û–¥–∏–Ω –∏–ª–∏ –æ–±–∞ –ø–æ–ª—é—Å–∞ –æ—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –º–æ–¥–µ–ª–∏: {', '.join(oov_tokens)}")
#         else:
#             st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

#     # --- 4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 2D/3D –ø—Ä–æ–µ–∫—Ü–∏–π ---
#     st.header("4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 2D/3D –ø—Ä–æ–µ–∫—Ü–∏–π")
#     st.write("–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ç–æ–∫–µ–Ω—ã –≤ 2D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º UMAP.")

#     num_tokens_for_viz = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:", 50, 500, 200)

#     if st.button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å 2D –ø—Ä–æ–µ–∫—Ü–∏—é (UMAP)"):
#         if model_wv:
#             try:
#                 # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≤—ã–±–æ—Ä–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤
#                 all_tokens = list(model_wv.key_to_index.keys())
#                 tokens_for_viz = all_tokens[:num_tokens_for_viz]
#                 vectors_for_viz = np.array([get_vector(model_wv, token, model_type) for token in tokens_for_viz])

#                 # –ü—Ä–∏–º–µ–Ω—è–µ–º UMAP
#                 reducer = umap.UMAP(n_components=2, random_state=42)
#                 embedding_2d = reducer.fit_transform(vectors_for_viz)

#                 # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
#                 viz_df = pd.DataFrame(embedding_2d, columns=['UMAP 1', 'UMAP 2'])
#                 viz_df['Token'] = tokens_for_viz

#                 # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
#                 entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
#                 fig = px.scatter(viz_df, x='UMAP 1', y='UMAP 2', text='Token',
#                                  title=f'2D UMAP –ø—Ä–æ–µ–∫—Ü–∏—è {num_tokens_for_viz} {entity_type}',
#                                  hover_name='Token')
#                 fig.update_traces(textposition='top center')
#                 st.plotly_chart(fig)

#             except Exception as e:
#                 st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ UMAP –ø—Ä–æ–µ–∫—Ü–∏–∏: {e}")
#         else:
#             st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

import streamlit as st
import gensim
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import umap
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
import os
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
@st.cache_resource
def load_model(model_path):
    try:
        model = None
        model_type = None
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if 'fasttext' in model_path.lower():
            model = gensim.models.FastText.load(model_path)
            model_type = 'fasttext'
        elif 'word2vec' in model_path.lower():
            model = gensim.models.Word2Vec.load(model_path)
            model_type = 'word2vec'
        elif 'doc2vec' in model_path.lower() or 'd2v' in model_path.lower():
            model = gensim.models.Doc2Vec.load(model_path)
            model_type = 'doc2vec'
        else:
            st.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—É—Ç–∏: {model_path}")
            return None, None

        # –î–ª—è Doc2Vec –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º –≤—ã–±–æ—Ä –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Å–ª–æ–≤ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if model_type == 'doc2vec':
            if hasattr(model, 'wv') and len(model.wv) > 0:
                return model.wv, 'doc2vec_words'
            elif hasattr(model, 'dv') and len(model.dv) > 0:
                return model.dv, 'doc2vec_docs'
            else:
                st.error("Doc2Vec –º–æ–¥–µ–ª—å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤, –Ω–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                return None, None
        elif model and hasattr(model, 'wv'):
            return model.wv, model_type
        else:
            st.error(f"–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏ –Ω–µ –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ '.wv'. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–∏–ø –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.")
            return None, None

    except FileNotFoundError:
        st.error(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {model_path}")
        return None, None
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_path}: {e}")
        st.exception(e)
        return None, None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è —Ç–æ–∫–µ–Ω–∞ –≤ –º–æ–¥–µ–ª–∏
def token_in_model(model, token, model_type):
    if model_type == 'doc2vec_docs':
        return token in model
    else:
        return token in model

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞
def get_vector(model, token, model_type):
    if model_type == 'doc2vec_docs':
        return model[token]
    else:
        return model[token]

# --- Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ---
st.title("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤")

st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")

# –ü–æ–∏—Å–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ü–µ–ª–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
path_to_models = os.path.join("–¢–µ–∫—Å—Ç –ê–Ω–∞–ª–∏–∑", "models")
model_files = [f for f in os.listdir(path_to_models) if f.endswith('.model')]
selected_model_path = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:", model_files)

model_wv = None
model_type = None

if selected_model_path:
    full_model_path = os.path.join(path_to_models, selected_model_path)
    model_wv, model_type = load_model(full_model_path)

if model_wv is None:
    st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è.")
else:
    st.success(f"–ú–æ–¥–µ–ª—å '{selected_model_path}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –¢–∏–ø: {model_type}. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(model_wv)}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è Doc2Vec –º–æ–¥–µ–ª–µ–π —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    if model_type == 'doc2vec_docs':
        st.info("‚ö†Ô∏è –ó–∞–≥—Ä—É–∂–µ–Ω–∞ Doc2Vec –º–æ–¥–µ–ª—å —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –†–∞–±–æ—Ç–∞–π—Ç–µ —Å —Ç–µ–≥–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤.")

    # --- 1. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ ---
    st.header("1. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞")
    
    if model_type == 'doc2vec_docs':
        st.write("–í–≤–µ–¥–∏—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–µ–≥–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'doc1 - doc2 + doc3'")
    else:
        st.write("–í–≤–µ–¥–∏—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ '—Å–ª–æ–≤–æ1 - —Å–ª–æ–≤–æ2 + —Å–ª–æ–≤–æ3' (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏).")

    default_example = "–ø—É—Ç–∏–Ω - –º—É–∂—á–∏–Ω + –∂–µ–Ω—â–∏–Ω" if model_type != 'doc2vec_docs' else "DOC_1 - DOC_2 + DOC_3"
    arithmetic_input = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ:", default_example)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏ –¥–ª—è –æ—Ç—á—ë—Ç–∞
    arithmetic_results = None

    if st.button("–í—ã—á–∏—Å–ª–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—É—é –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫—É"):
        if model_wv:
            try:
                parts = arithmetic_input.split()
                positive = []
                negative = []
                current_op = '+'
                valid_input = True

                for part in parts:
                    if part == '+':
                        current_op = '+'
                    elif part == '-':
                        current_op = '-'
                    elif token_in_model(model_wv, part, model_type):
                        if current_op == '+':
                            positive.append(part)
                        else:
                            negative.append(part)
                    else:
                        st.warning(f"–¢–æ–∫–µ–Ω '{part}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–≤–µ—Ä–µ–Ω.")
                        valid_input = False
                        break

                if valid_input and (positive or negative):
                    st.write(f"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ: {' + '.join(positive)} - {' - '.join(negative)}")
                    try:
                        if positive:
                            result_vector = get_vector(model_wv, positive[0], model_type).copy()
                        else:
                            result_vector = np.zeros(model_wv.vector_size)
                        
                        for token in positive[1:]:
                            result_vector += get_vector(model_wv, token, model_type).copy()
                        for token in negative:
                            result_vector -= get_vector(model_wv, token, model_type).copy()

                        st.write("–ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞:")
                        
                        if model_type == 'doc2vec_docs':
                            similarities = []
                            for doc_tag in list(model_wv.key_to_index.keys())[:100]:
                                doc_vector = get_vector(model_wv, doc_tag, model_type)
                                similarity = cosine_similarity([result_vector], [doc_vector])[0][0]
                                similarities.append((doc_tag, similarity))
                            
                            similarities.sort(key=lambda x: x[1], reverse=True)
                            arithmetic_results = similarities[:10]
                            for token, similarity in arithmetic_results:
                                st.write(f"- {token} (–°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f})")
                        else:
                            arithmetic_results = model_wv.most_similar(positive=positive, negative=negative, topn=10)
                            for word, similarity in arithmetic_results:
                                st.write(f"- {word} (–°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f})")

                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –∏–ª–∏ –ø–æ–∏—Å–∫–µ —Å–æ—Å–µ–¥–µ–π: {e}")

                elif valid_input:
                    st.warning("–í–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏.")

            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–≤–æ–¥–∞: {e}")
        else:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    # --- 2. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º ---
    st.header("2. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º")

    label_1 = "–¢–µ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ 1:" if model_type == 'doc2vec_docs' else "–°–ª–æ–≤–æ 1:"
    label_2 = "–¢–µ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ 2:" if model_type == 'doc2vec_docs' else "–°–ª–æ–≤–æ 2:"
    
    default_1 = "DOC_1" if model_type == 'doc2vec_docs' else "–ø—É—Ç–∏–Ω"
    default_2 = "DOC_2" if model_type == 'doc2vec_docs' else "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç"
    
    token1_sim = st.text_input(label_1, default_1)
    token2_sim = st.text_input(label_2, default_2)

    similarity_result = None

    if st.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"):
        if model_wv:
            if (token_in_model(model_wv, token1_sim, model_type) and 
                token_in_model(model_wv, token2_sim, model_type)):
                try:
                    if model_type == 'doc2vec_docs':
                        vec1 = get_vector(model_wv, token1_sim, model_type)
                        vec2 = get_vector(model_wv, token2_sim, model_type)
                        similarity_result = cosine_similarity([vec1], [vec2])[0][0]
                    else:
                        similarity_result = model_wv.similarity(token1_sim, token2_sim)
                    
                    entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
                    st.write(f"–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É '{token1_sim}' –∏ '{token2_sim}': {similarity_result:.4f}")
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
            else:
                oov_tokens = [t for t in [token1_sim, token2_sim] if not token_in_model(model_wv, t, model_type)]
                entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
                st.warning(f"–û–¥–∏–Ω –∏–ª–∏ –æ–±–∞ {entity_type} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –º–æ–¥–µ–ª–∏: {', '.join(oov_tokens)}")
        else:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    # --- 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–µ–π ---
    st.header("3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–µ–π")
    
    if model_type == 'doc2vec_docs':
        st.write("–í—ã–±–µ—Ä–∏—Ç–µ –¥–≤–∞ —Ç–µ–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Å–∏.")
        pole1_label = "–ü–æ–ª—é—Å 1 (—Ç–µ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞):"
        pole2_label = "–ü–æ–ª—é—Å 2 (—Ç–µ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞):"
        default_pole1 = "DOC_1"
        default_pole2 = "DOC_2"
    else:
        st.write("–í—ã–±–µ—Ä–∏—Ç–µ –¥–≤–∞ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Å–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥—Ä—É–≥–∏—Ö —Å–ª–æ–≤.")
        pole1_label = "–ü–æ–ª—é—Å 1:"
        pole2_label = "–ü–æ–ª—é—Å 2:"
        default_pole1 = "–º—É–∂—á–∏–Ω"
        default_pole2 = "–∂–µ–Ω—â–∏–Ω"

    axis_token1 = st.text_input(pole1_label, default_pole1)
    axis_token2 = st.text_input(pole2_label, default_pole2)
    num_tokens_on_axis = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏:", 10, 100, 30)

    if st.button("–í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Å—å"):
        if model_wv:
            if (token_in_model(model_wv, axis_token1, model_type) and 
                token_in_model(model_wv, axis_token2, model_type)):
                try:
                    axis_vector = (get_vector(model_wv, axis_token2, model_type) - 
                                 get_vector(model_wv, axis_token1, model_type))

                    all_tokens = list(model_wv.key_to_index.keys())
                    start_idx = min(50, len(all_tokens) - num_tokens_on_axis)
                    tokens_to_project = all_tokens[start_idx:start_idx + num_tokens_on_axis]

                    projections = []
                    token_labels = []
                    for token in tokens_to_project:
                        if token_in_model(model_wv, token, model_type):
                            token_vector = get_vector(model_wv, token, model_type)
                            norm_axis_vector = axis_vector / np.linalg.norm(axis_vector)
                            projection_value = np.dot(token_vector, norm_axis_vector)
                            projections.append(projection_value)
                            token_labels.append(token)

                    projection_df = pd.DataFrame({'Token': token_labels, 'Projection': projections})
                    projection_df = projection_df.sort_values('Projection')

                    entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
                    fig = px.bar(projection_df, x='Projection', y='Token', orientation='h',
                                 title=f'–ü—Ä–æ–µ–∫—Ü–∏—è {entity_type} –Ω–∞ –æ—Å—å "{axis_token1}" - "{axis_token2}"')
                    st.plotly_chart(fig)

                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å–∏: {e}")
            else:
                oov_tokens = [t for t in [axis_token1, axis_token2] if not token_in_model(model_wv, t, model_type)]
                entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
                st.warning(f"–û–¥–∏–Ω –∏–ª–∏ –æ–±–∞ –ø–æ–ª—é—Å–∞ –æ—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –º–æ–¥–µ–ª–∏: {', '.join(oov_tokens)}")
        else:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    # --- 4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 2D/3D –ø—Ä–æ–µ–∫—Ü–∏–π ---
    st.header("4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 2D/3D –ø—Ä–æ–µ–∫—Ü–∏–π")
    st.write("–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ç–æ–∫–µ–Ω—ã –≤ 2D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º UMAP.")

    num_tokens_for_viz = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:", 50, 500, 200)

    umap_results = None

    if st.button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å 2D –ø—Ä–æ–µ–∫—Ü–∏—é (UMAP)"):
        if model_wv:
            try:
                all_tokens = list(model_wv.key_to_index.keys())
                tokens_for_viz = all_tokens[:num_tokens_for_viz]
                vectors_for_viz = np.array([get_vector(model_wv, token, model_type) for token in tokens_for_viz])

                reducer = umap.UMAP(n_components=2, random_state=42)
                embedding_2d = reducer.fit_transform(vectors_for_viz)

                viz_df = pd.DataFrame(embedding_2d, columns=['UMAP 1', 'UMAP 2'])
                viz_df['Token'] = tokens_for_viz
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç—á—ë—Ç–∞
                umap_results = {
                    'dataframe': viz_df,
                    'tokens': tokens_for_viz,
                    'vectors': vectors_for_viz
                }

                entity_type = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if model_type == 'doc2vec_docs' else "—Å–ª–æ–≤"
                fig = px.scatter(viz_df, x='UMAP 1', y='UMAP 2', text='Token',
                                 title=f'2D UMAP –ø—Ä–æ–µ–∫—Ü–∏—è {num_tokens_for_viz} {entity_type}',
                                 hover_name='Token')
                fig.update_traces(textposition='top center')
                st.plotly_chart(fig)

            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ UMAP –ø—Ä–æ–µ–∫—Ü–∏–∏: {e}")
        else:
            st.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    # --- 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á—ë—Ç–∞ ---
    st.header("5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á—ë—Ç–∞")
    
    # –°–æ–∑–¥–∞–µ–º –≤–∫–ª–∞–¥–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –æ—Ç—á—ë—Ç–∞
    report_tabs = st.tabs([
        "üìä –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", 
        "üßÆ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞", 
        "üìà –¢–æ—á–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–æ–≥–∏–π",
        "üî• Heatmap –±–ª–∏–∑–æ—Å—Ç–µ–π",
        "üîç –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
    ])
    
    with report_tabs[0]:
        st.subheader("–°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è", len(model_wv))
        with col2:
            st.metric("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤", model_wv.vector_size)
        with col3:
            st.metric("–¢–∏–ø –º–æ–¥–µ–ª–∏", model_type)
        
        # –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤
        st.subheader("–ü—Ä–∏–º–µ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤")
        example_tokens = list(model_wv.key_to_index.keys())[:20]
        st.write(", ".join(example_tokens))
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç —Ç–æ–∫–µ–Ω–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        if hasattr(model_wv, 'key_to_index'):
            st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç —Ç–æ–∫–µ–Ω–æ–≤")
            token_counts = pd.DataFrame({
                'Token': list(model_wv.key_to_index.keys())[:50],
                'Index': list(range(50))
            })
            fig = px.bar(token_counts, x='Index', y='Index', 
                        title='–ü–µ—Ä–≤—ã–µ 50 —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ (—É—Å–ª–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞)')
            st.plotly_chart(fig)
    
    with report_tabs[1]:
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏")
        
        if arithmetic_results:
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if isinstance(arithmetic_results[0], tuple):
                words = [item[0] for item in arithmetic_results]
                similarities = [item[1] for item in arithmetic_results]
            else:
                words = [item[0] for item in arithmetic_results]
                similarities = [item[1] for item in arithmetic_results]
            
            results_df = pd.DataFrame({
                '–¢–æ–∫–µ–Ω': words,
                '–°—Ö–æ–¥—Å—Ç–≤–æ': similarities
            })
            
            fig = px.bar(results_df, x='–¢–æ–∫–µ–Ω', y='–°—Ö–æ–¥—Å—Ç–≤–æ',
                        title='–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏',
                        color='–°—Ö–æ–¥—Å—Ç–≤–æ',
                        color_continuous_scale='Viridis')
            st.plotly_chart(fig)
            
            # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.dataframe(results_df.style.format({'–°—Ö–æ–¥—Å—Ç–≤–æ': '{:.4f}'}))
        else:
            st.info("–í—ã–ø–æ–ª–Ω–∏—Ç–µ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫—É –≤ —Ä–∞–∑–¥–µ–ª–µ 1, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–¥–µ—Å—å.")
    
    with report_tabs[2]:
        st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–æ–≥–∏–π")
        
        # –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –∞–Ω–∞–ª–æ–≥–∏–π
        analogy_tests = [
            ["–º—É–∂—á–∏–Ω", "–∂–µ–Ω—â–∏–Ω", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "—á–µ–º–ø–∏–æ–Ω–∞—Ç"],
            ["–ø–∞—Ä–∏–∂", "—Ñ—Ä–∞–Ω—Ü–∏—è", "–º–æ—Å–∫–≤–∞", "—Ä–æ—Å—Å–∏—è"],
            ["—Ö–æ–ª–æ–¥–Ω—ã–π", "—Ö–æ–ª–æ–¥–Ω–µ–µ", "–≥–æ—Ä—è—á–∏–π", "–≥–æ—Ä—è—á–µ–µ"],
            ["—Å–æ–±–∞–∫–∞", "—â–µ–Ω–æ–∫", "–∫–æ—à–∫–∞", "–∫–æ—Ç–µ–Ω–æ–∫"]
        ]
        
        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏–∏
        st.write("–î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ —Ç–µ—Å—Ç—ã –∞–Ω–∞–ª–æ–≥–∏–π:")
        custom_analogy = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–æ–≥–∏—é (—Ñ–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–æ1 —Å–ª–æ–≤–æ2 —Å–ª–æ–≤–æ3 —Å–ª–æ–≤–æ4):", 
                                      "–º—É–∂—á–∏–Ω–∞ –∂–µ–Ω—â–∏–Ω–∞ –∫–æ—Ä–æ–ª—å –∫–æ—Ä–æ–ª–µ–≤–∞")
        
        if st.button("–î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç"):
            parts = custom_analogy.split()
            if len(parts) == 4:
                analogy_tests.append(parts)
                st.success("–¢–µ—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω!")
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤
        correct = 0
        total = 0
        results = []
        
        for test in analogy_tests:
            if all(token_in_model(model_wv, word, model_type) for word in test):
                total += 1
                try:
                    # –í—ã—á–∏—Å–ª—è–µ–º –∞–Ω–∞–ª–æ–≥–∏—é: word1 - word2 + word3 ‚âà word4
                    positive = [test[0], test[2]]  # word1 –∏ word3
                    negative = [test[1]]           # word2
                    
                    # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏
                    similar_words = model_wv.most_similar(
                        positive=positive, 
                        negative=negative, 
                        topn=5
                    )
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ü–µ–ª–µ–≤–æ–µ —Å–ª–æ–≤–æ –≤ —Ç–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
                    target_word = test[3]
                    found = any(target_word in word for word, score in similar_words)
                    
                    if found:
                        correct += 1
                        results.append({
                            '–ê–Ω–∞–ª–æ–≥–∏—è': f"{test[0]} - {test[1]} + {test[2]} ‚âà {test[3]}",
                            '–†–µ–∑—É–ª—å—Ç–∞—Ç': '‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ',
                            '–¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤': [word for word, score in similar_words[:5]]
                        })
                    else:
                        results.append({
                            '–ê–Ω–∞–ª–æ–≥–∏—è': f"{test[0]} - {test[1]} + {test[2]} ‚âà {test[3]}",
                            '–†–µ–∑—É–ª—å—Ç–∞—Ç': '‚ùå –û—à–∏–±–∫–∞',
                            '–¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤': [word for word, score in similar_words[:5]]
                        })
                        
                except Exception as e:
                    results.append({
                        '–ê–Ω–∞–ª–æ–≥–∏—è': f"{test[0]} - {test[1]} + {test[2]} ‚âà {test[3]}",
                        '–†–µ–∑—É–ª—å—Ç–∞—Ç': f'‚ö†Ô∏è –û—à–∏–±–∫–∞: {str(e)}',
                        '–¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤': []
                    })
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if total > 0:
            accuracy = correct / total
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("–¢–æ—á–Ω–æ—Å—Ç—å", f"{accuracy:.2%}")
            with col2:
                st.metric("–ü—Ä–æ–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤", f"{correct}/{total}")
            
            # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
            fig = px.pie(
                values=[correct, total - correct],
                names=['–ü—Ä–∞–≤–∏–ª—å–Ω–æ', '–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ'],
                title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤ –∞–Ω–∞–ª–æ–≥–∏–π'
            )
            st.plotly_chart(fig)
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.subheader("–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            for result in results:
                with st.expander(result['–ê–Ω–∞–ª–æ–≥–∏—è']):
                    st.write(f"**–†–µ–∑—É–ª—å—Ç–∞—Ç:** {result['–†–µ–∑—É–ª—å—Ç–∞—Ç']}")
                    if result['–¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤']:
                        st.write("**–¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**")
                        for i, word in enumerate(result['–¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤'], 1):
                            st.write(f"{i}. {word}")
        else:
            st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω —Ç–µ—Å—Ç –∞–Ω–∞–ª–æ–≥–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏.")
    
    with report_tabs[3]:
        st.subheader("Heatmap —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–∏–∑–æ—Å—Ç–µ–π")
        
        # –í—ã–±–æ—Ä —Å–ª–æ–≤ –¥–ª—è heatmap
        default_words = "–ø—É—Ç–∏–Ω, –º–µ–¥–≤–µ–¥–µ–≤, –Ω–∞–≤–∞–ª—å–Ω—ã–π, –∑–µ—Ä–∫–∞–ª–æ, —Å–æ–±–∞–∫–∞, –∫–æ—à–∫–∞, –º–∞—à–∏–Ω–∞, –¥–æ–º, —Ä–∞–±–æ—Ç–∞, –¥–µ–Ω—å–≥–∏"
        selected_words = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–∞ –¥–ª—è heatmap (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):", 
            default_words,
            height=100
        )
        
        words_list = [word.strip() for word in selected_words.split(',') if word.strip()]
        valid_words = [word for word in words_list if token_in_model(model_wv, word, model_type)]
        
        if len(valid_words) >= 2:
            # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤
            similarity_matrix = np.zeros((len(valid_words), len(valid_words)))
            
            for i, word1 in enumerate(valid_words):
                for j, word2 in enumerate(valid_words):
                    if i == j:
                        similarity_matrix[i, j] = 1.0
                    else:
                        try:
                            if model_type == 'doc2vec_docs':
                                vec1 = get_vector(model_wv, word1, model_type)
                                vec2 = get_vector(model_wv, word2, model_type)
                                similarity_matrix[i, j] = cosine_similarity([vec1], [vec2])[0][0]
                            else:
                                similarity_matrix[i, j] = model_wv.similarity(word1, word2)
                        except:
                            similarity_matrix[i, j] = 0.0
            
            # –°–æ–∑–¥–∞–µ–º heatmap
            fig = px.imshow(
                similarity_matrix,
                x=valid_words,
                y=valid_words,
                color_continuous_scale='RdBu_r',
                title='Heatmap —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–∏–∑–æ—Å—Ç–µ–π',
                aspect="auto"
            )
            
            fig.update_layout(
                xaxis_title="–°–ª–æ–≤–∞",
                yaxis_title="–°–ª–æ–≤–∞"
            )
            
            st.plotly_chart(fig)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤")
            flat_similarities = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("–ú–∞–∫—Å. —Å—Ö–æ–¥—Å—Ç–≤–æ", f"{np.max(flat_similarities):.4f}")
            with col2:
                st.metric("–ú–∏–Ω. —Å—Ö–æ–¥—Å—Ç–≤–æ", f"{np.min(flat_similarities):.4f}")
            with col3:
                st.metric("–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ", f"{np.mean(flat_similarities):.4f}")
                
        else:
            st.warning("–í–≤–µ–¥–∏—Ç–µ –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ –º–æ–¥–µ–ª–∏.")
    
    with report_tabs[4]:
        st.subheader("–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ 2D –ø—Ä–æ–µ–∫—Ü–∏–π")
        
        if umap_results is not None:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            n_clusters = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:", 2, 10, 4)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            clusters = kmeans.fit_predict(umap_results['vectors'])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö –≤ DataFrame
            cluster_df = umap_results['dataframe'].copy()
            cluster_df['–ö–ª–∞—Å—Ç–µ—Ä'] = clusters.astype(str)
            cluster_df['–†–∞–∑–º–µ—Ä_—Ç–æ—á–∫–∏'] = 5  # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
            fig = px.scatter(
                cluster_df, 
                x='UMAP 1', 
                y='UMAP 2', 
                color='–ö–ª–∞—Å—Ç–µ—Ä',
                hover_name='Token',
                title=f'2D UMAP –ø—Ä–æ–µ–∫—Ü–∏—è —Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π (K-means, k={n_clusters})',
                size='–†–∞–∑–º–µ—Ä_—Ç–æ—á–∫–∏',
                size_max=8
            )
            
            st.plotly_chart(fig)
            
            # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            st.subheader("–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            
            for cluster_id in range(n_clusters):
                cluster_tokens = cluster_df[cluster_df['–ö–ª–∞—Å—Ç–µ—Ä'] == str(cluster_id)]['Token'].tolist()
                with st.expander(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({len(cluster_tokens)} —Ç–æ–∫–µ–Ω–æ–≤)"):
                    st.write(", ".join(cluster_tokens[:20]))  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20 —Ç–æ–∫–µ–Ω–æ–≤
                    if len(cluster_tokens) > 20:
                        st.write(f"... –∏ –µ—â–µ {len(cluster_tokens) - 20} —Ç–æ–∫–µ–Ω–æ–≤")
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            from sklearn.metrics import silhouette_score
            
            try:
                silhouette_avg = silhouette_score(umap_results['vectors'], clusters)
                st.metric("Silhouette Score", f"{silhouette_avg:.4f}")
                
                if silhouette_avg > 0.5:
                    st.success("–•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
                elif silhouette_avg > 0.25:
                    st.warning("–£–º–µ—Ä–µ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
                else:
                    st.error("–ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
                    
            except Exception as e:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
                
        else:
            st.info("–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ 2D –ø—Ä–æ–µ–∫—Ü–∏—é –≤ —Ä–∞–∑–¥–µ–ª–µ 4, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑.")
    
    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞
    st.divider()
    if st.button("üìÑ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç PDF"):
        st.info("–§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF –æ—Ç—á—ë—Ç–∞ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ. –í—Å–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤—ã—à–µ.")