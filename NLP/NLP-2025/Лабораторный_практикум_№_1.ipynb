{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daarlens/ProBook/blob/main/NLP/NLP-2025/%D0%9B%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D1%83%D0%BC_%E2%84%96_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ **–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–π –ø—Ä–∞–∫—Ç–∏–∫—É–º ‚Ññ 1**  \n",
        "# **–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤**\n",
        "\n",
        "\n",
        "\n",
        "**–ö–∞—Ñ–µ–¥—Ä–∞:** –ö–∞—Ñ–µ–¥—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è  \n",
        "**–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞:** –û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞  \n",
        "**–£—Ä–æ–≤–µ–Ω—å:** –ú–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞, 2 –∫—É—Ä—Å  \n",
        "**–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å:** –ê—Ä–∞–±–æ–≤ –ú—É–ª–ª–æ—à–∞—Ä–∞—Ñ –ö—É—Ä–±–æ–Ω–æ–≤–∏—á  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## üéØ 1. –¶–µ–ª–∏ –∏ –∑–∞–¥–∞—á–∏ —Ä–∞–±–æ—Ç—ã\n",
        "\n",
        "### **–¶–µ–ª—å:**\n",
        "–ü–æ–ª—É—á–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –ø–æ–ª–Ω–æ–º —Ü–∏–∫–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ (Natural Language Processing, NLP) ‚Äî –æ—Ç —Å–±–æ—Ä–∞ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å-–º–æ–¥–µ–ª–µ–π. –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–≤—ã–∫–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∑–∞–¥–∞—á–µ NLP ‚Äî —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.\n",
        "\n",
        "### **–ó–∞–¥–∞—á–∏:**\n",
        "1. –û—Å–≤–æ–∏—Ç—å –º–µ—Ç–æ–¥–∏–∫–∏ —Å–±–æ—Ä–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –∏–∑ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.\n",
        "2. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, —Å—Ç–µ–º–º–∏–Ω–≥–∞ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏.\n",
        "3. –ü—Ä–∏–æ–±—Ä–µ—Å—Ç–∏ –Ω–∞–≤—ã–∫–∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (BPE, WordPiece, Unigram).\n",
        "4. –í—ã—Ä–∞–±–æ—Ç–∞—Ç—å —É–º–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫.\n",
        "5. –ù–∞—É—á–∏—Ç—å—Å—è –æ—Ñ–æ—Ä–º–ª—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –≤–∏–¥–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–≥–æ API-—Å–µ—Ä–≤–∏—Å–∞.\n",
        "6. –û—Å–≤–æ–∏—Ç—å –ø—Ä–æ—Ü–µ–¥—É—Ä—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö.\n",
        "\n",
        "\n",
        "\n",
        "## üìö 2. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏\n",
        "\n",
        "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–∞—â–∏–µ –µ–¥–∏–Ω–∏—Ü—ã (—Ç–æ–∫–µ–Ω—ã), —è–≤–ª—è–µ—Ç—Å—è –±–∞–∑–æ–≤–æ–π –æ–ø–µ—Ä–∞—Ü–∏–µ–π –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ö–∞—á–µ—Å—Ç–≤–æ –µ—ë –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —ç—Ç–∞–ø–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è, –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è, –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥).\n",
        "\n",
        "–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ç—Ä–∏ –∫–ª–∞—Å—Å–∞ –º–µ—Ç–æ–¥–æ–≤:\n",
        "\n",
        "1. **–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã:**  \n",
        "   –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è ‚Äî –ø—Ä–æ—Å—Ç—ã–µ, –Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∫ —à—É–º—É.\n",
        "\n",
        "2. **–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã:**  \n",
        "   - *–°—Ç–µ–º–º–∏–Ω–≥* ‚Äî –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã –∫ –æ—Å–Ω–æ–≤–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ‚Äú—Ä–∞–±–æ—Ç–∞–ª–∏‚Äù ‚Üí ‚Äú—Ä–∞–±–æ—Ç‚Äù).  \n",
        "   - *–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è* ‚Äî –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π (—Å–ª–æ–≤–∞—Ä–Ω–æ–π) —Ñ–æ—Ä–º–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ‚Äú—Ä–∞–±–æ—Ç–∞–ª–∏‚Äù ‚Üí ‚Äú—Ä–∞–±–æ—Ç–∞—Ç—å‚Äù).\n",
        "\n",
        "3. **–ü–æ–¥—Å–ª–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã (Subword Tokenization):**  \n",
        "   –ê–ª–≥–æ—Ä–∏—Ç–º—ã, –æ–±—É—á–∞—é—â–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Å–ª–æ–≤ –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∫–æ—Ä–ø—É—Å–∞ (BPE, WordPiece, Unigram). –ü–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å–ª–æ–≤–∞—Ä–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã, –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–≤—à–∏–µ—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è (OOV ‚Äî Out-Of-Vocabulary).\n",
        "\n",
        "\n",
        "\n",
        "## üß™ 3. –ú–µ—Ç–æ–¥–∏–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏ –ø–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
        "\n",
        "\n",
        "\n",
        "### **3.1. –≠—Ç–∞–ø 1. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤**\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –°–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π –∫–æ—Ä–ø—É—Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.\n",
        "\n",
        "#### **–£–∫–∞–∑–∞–Ω–∏—è –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é:**\n",
        "\n",
        "- **–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:**  \n",
        "  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª—ã –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∞–≥–µ–Ω—Ç—Å—Ç–≤ –∏ –ø–æ—Ä—Ç–∞–ª–æ–≤:  \n",
        "  `ria.ru`, `tass.ru`, `lenta.ru`, `meduza.io`, `kommersant.ru` –∏ –¥—Ä.\n",
        "\n",
        "  > üí° **–ò–Ω–∫–ª—é–∑–∏–≤–Ω–æ–µ –∑–∞–¥–∞–Ω–∏–µ:**  \n",
        "  > –ï—Å–ª–∏ –≤—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–º –Ω–∞—Ä–æ–¥–æ–≤ –†–§ (—Ç–∞—Ç–∞—Ä—ã, –±–∞—à–∫–∏—Ä—ã, —É–¥–º—É—Ä—Ç—ã, —á—É–≤–∞—à, –º–∞—Ä–∏–π—Ü—ã –∏ –¥—Ä.), –ø–æ—Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å–∞–π—Ç—ã –Ω–∞ —Å–≤–æ—ë–º —Ä–æ–¥–Ω–æ–º —è–∑—ã–∫–µ. –í–∞—à–∞ —Ä–∞–±–æ—Ç–∞ –ø–æ–º–æ–∂–µ—Ç —Ä–∞–∑–≤–∏–≤–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —è–∑—ã–∫–æ–≤–æ–µ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–µ –†–æ—Å—Å–∏–∏.\n",
        "\n",
        "- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏):**  \n",
        "  - –ó–∞–≥–æ–ª–æ–≤–æ–∫  \n",
        "  - –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç  \n",
        "  - –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏  \n",
        "  - URL-–∞–¥—Ä–µ—Å  \n",
        "  - –ö–∞—Ç–µ–≥–æ—Ä–∏—è/—Ä—É–±—Ä–∏–∫–∞ (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏)\n",
        "\n",
        "- **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π:**  \n",
        "  - `requests` + `BeautifulSoup4` ‚Äî –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü  \n",
        "  - `selenium` ‚Äî –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤\n",
        "\n",
        "- **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–æ—Ä–ø—É—Å—É:**  \n",
        "  - –û–±—â–∏–π –æ–±—ä—ë–º: **–Ω–µ –º–µ–Ω–µ–µ 50 000 —Å–ª–æ–≤**  \n",
        "  - –§–æ—Ä–º–∞—Ç —Ö—Ä–∞–Ω–µ–Ω–∏—è: **`JSONL`** (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–π JSON-–æ–±—ä–µ–∫—Ç —Å–æ —Å—Ç–∞—Ç—å—ë–π)\n",
        "\n",
        "\n",
        "\n",
        "### **3.2. –≠—Ç–∞–ø 2. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞**\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–æ–¥—É–ª—å –ø–µ—Ä–≤–∏—á–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –Ω–µ—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ –µ–≥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
        "\n",
        "#### **–£–∫–∞–∑–∞–Ω–∏—è –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é:**\n",
        "\n",
        "–°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥—É–ª—å `text_cleaner.py`, –≤—ã–ø–æ–ª–Ω—è—é—â–∏–π:\n",
        "\n",
        "- –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ä–∞–∑–º–µ—Ç–∫–∏, —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –±–ª–æ–∫–æ–≤\n",
        "- –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—é –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
        "- –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äî –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏)\n",
        "- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Å—Ç–æ–ø-—Å–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º `nltk.corpus.stopwords`)\n",
        "\n",
        "\n",
        "### **3.3. –≠—Ç–∞–ø 3. –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –º–æ–¥—É–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏**\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–π –º–æ–¥—É–ª—å `universal_preprocessor.py` –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∫ –µ–¥–∏–Ω–æ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç—É –ø–µ—Ä–µ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π.\n",
        "\n",
        "#### **–£–∫–∞–∑–∞–Ω–∏—è –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é:**\n",
        "\n",
        "–ú–æ–¥—É–ª—å –¥–æ–ª–∂–µ–Ω –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å:\n",
        "\n",
        "- –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–æ–±–µ–ª–æ–≤\n",
        "- –ó–∞–º–µ–Ω—É —á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö, URL-–∞–¥—Ä–µ—Å–æ–≤ –∏ email –Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã:  \n",
        "  `<NUM>`, `<URL>`, `<EMAIL>`\n",
        "- –û–±—Ä–∞–±–æ—Ç–∫—É –æ–±—â–µ—è–∑—ã–∫–æ–≤—ã—Ö –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä: ‚Äú—Ç.–µ.‚Äù ‚Üí ‚Äú—Ç–æ –µ—Å—Ç—å‚Äù, ‚Äú–≥.‚Äù ‚Üí ‚Äú–≥–æ–¥‚Äù)\n",
        "\n",
        "> üõ†Ô∏è **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫—É `re` (—Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è) –¥–ª—è –≥–∏–±–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∞–≤–∏–ª.\n",
        "\n",
        "\n",
        "\n",
        "### **3.4. –≠—Ç–∞–ø 4. –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏**\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –ü—Ä–æ–≤–µ—Å—Ç–∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–µ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤.\n",
        "\n",
        "#### **–ü–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:**\n",
        "\n",
        "1. **–ú–µ—Ç–æ–¥—ã:**\n",
        "   - *–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:*  \n",
        "     - –ù–∞–∏–≤–Ω–∞—è (–ø–æ –ø—Ä–æ–±–µ–ª–∞–º)  \n",
        "     - –ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π  \n",
        "     - –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏: `nltk`, `spacy`, `razdel` (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ ‚Äî –æ—Å–æ–±–µ–Ω–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)\n",
        "   - *–°—Ç–µ–º–º–∏–Ω–≥:*  \n",
        "     - `PorterStemmer`, `SnowballStemmer` (—Ä—É—Å—Å–∫–∏–π)\n",
        "   - *–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è:*  \n",
        "     - `pymorphy2`, `spacy` (`ru_core_news_sm`)\n",
        "\n",
        "2. **–ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏:**\n",
        "   - **–û–±—ä—ë–º —Å–ª–æ–≤–∞—Ä—è** ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–º–µ–Ω—å—à–µ ‚Üí –∫–æ–º–ø–∞–∫—Ç–Ω–µ–µ)\n",
        "   - **–î–æ–ª—è OOV (Out-of-Vocabulary)** ‚Äî % —Å–ª–æ–≤, –Ω–µ –≤–æ—à–µ–¥—à–∏—Ö –≤ —Å–ª–æ–≤–∞—Ä—å (–Ω–∏–∂–µ ‚Üí –ª—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ)\n",
        "   - **–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏** ‚Äî –≤—Ä–µ–º—è –Ω–∞ 1000 —Å—Ç–∞—Ç–µ–π (–≤–∞–∂–Ω–æ –¥–ª—è production)\n",
        "   - **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å** ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ª–∏ —Å–º—ã—Å–ª?  \n",
        "     ‚Üí *–ú–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ/–ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–ª–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –Ω–∞ –≤—ã–±–æ—Ä–∫–µ.*\n",
        "\n",
        "3. **–û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**  \n",
        "   –°–≤–µ–¥–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É `tokenization_metrics.csv` –∏ –¥–æ–±–∞–≤—å—Ç–µ –∞–Ω–∞–ª–∏–∑ –≤ –æ—Ç—á—ë—Ç.\n",
        "\n",
        "\n",
        "\n",
        "### **3.5. –≠—Ç–∞–ø 5. –û–±—É—á–µ–Ω–∏–µ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏**\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –û–±—É—á–∏—Ç—å —Ç—Ä–∏ –ø–æ–¥—Å–ª–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –µ–¥–∏–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ –∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ –∏—Ö —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑.\n",
        "\n",
        "#### **–£–∫–∞–∑–∞–Ω–∏—è –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é:**\n",
        "\n",
        "1. **–ú–æ–¥–µ–ª–∏:**\n",
        "   - Byte Pair Encoding (BPE)\n",
        "   - WordPiece\n",
        "   - Unigram Language Model\n",
        "\n",
        "2. **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:**  \n",
        "   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: `tokenizers` (Hugging Face) –∏–ª–∏ `sentencepiece`\n",
        "\n",
        "3. **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:**\n",
        "   - –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: **8 000 ‚Äì 32 000 —Ç–æ–∫–µ–Ω–æ–≤** (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π)\n",
        "   - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Ç–æ–∫–µ–Ω–∞: 2‚Äì5\n",
        "\n",
        "4. **–ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏:**\n",
        "   - **–ü—Ä–æ—Ü–µ–Ω—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å–ª–æ–≤** ‚Äî –¥–æ–ª—è —Å–ª–æ–≤, —Ä–∞–∑–±–∏—Ç—ã—Ö –Ω–∞ 2+ –ø–æ–¥—Å–ª–æ–≤–∞ (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç ¬´–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å¬ª –º–æ–¥–µ–ª–∏)\n",
        "   - **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è** ‚Äî –æ—Ç–Ω–æ—à–µ–Ω–∏–µ —á–∏—Å–ª–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–ª–æ–≤ –∫ —á–∏—Å–ª—É —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "   - **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏** ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ –º–æ–¥–µ–ª—å –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "\n",
        "\n",
        "### **3.6. –≠—Ç–∞–ø 6. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞**\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.\n",
        "\n",
        "#### **–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**\n",
        "\n",
        "- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (—Å–≤–æ–π —Ñ–∞–π–ª –∏–ª–∏ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π)\n",
        "- –í—ã–±–æ—Ä —è–∑—ã–∫–∞ –∏ –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á—ë—Ç–∞ —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏:\n",
        "  - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤\n",
        "  - –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤\n",
        "  - –î–æ–ª—è OOV\n",
        "- –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ + –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ HTML/PDF\n",
        "\n",
        "#### **–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫ (–Ω–∞ –≤—ã–±–æ—Ä):**\n",
        "- `Streamlit`, `Gradio` ‚Äî –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "- `Flask` / `FastAPI` + `Jinja2` + `Plotly` ‚Äî –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏\n",
        "\n",
        "\n",
        "\n",
        "### **3.7. –≠—Ç–∞–ø 7. –ü—É–±–ª–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –≤ Hugging Face Hub**\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –û–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –ª—É—á—à–∏–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ.\n",
        "\n",
        "#### **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—é:**\n",
        "\n",
        "–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ **Model Card** (–∫–∞—Ä—Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏), —Å–æ–¥–µ—Ä–∂–∞—â—É—é:\n",
        "\n",
        "```markdown\n",
        "# [–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: Russian BPE Tokenizer 16k]\n",
        "\n",
        "## üóÉÔ∏è –ö–æ—Ä–ø—É—Å\n",
        "50k+ —Å–ª–æ–≤ —Å ria.ru, lenta.ru –∏ –¥—Ä. (2020‚Äì2025)\n",
        "\n",
        "## ‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "- –ê–ª–≥–æ—Ä–∏—Ç–º: BPE\n",
        "- –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: 16,000\n",
        "- Min frequency: 3\n",
        "\n",
        "## üìä –ú–µ—Ç—Ä–∏–∫–∏\n",
        "- OOV rate: 1.2%\n",
        "- Reconstruction accuracy: 99.8%\n",
        "- Compression ratio: 1.35\n",
        "\n",
        "## üíª –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"–≤–∞—à-username/–Ω–∞–∑–≤–∞–Ω–∏–µ-–º–æ–¥–µ–ª–∏\")\n",
        "tokens = tokenizer.tokenize(\"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\")\n",
        "```\n",
        "\n",
        "## üìú –õ–∏—Ü–µ–Ω–∑–∏—è\n",
        "MIT\n",
        "```\n",
        "\n",
        "> ‚úÖ –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å `transformers` –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ `AutoTokenizer`.\n",
        "\n",
        "\n",
        "## üîç 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è (–ø–æ –∂–µ–ª–∞–Ω–∏—é)\n",
        "\n",
        "1. **–ê–Ω–∞–ª–∏–∑ –∑–∞–∫–æ–Ω–∞ –¶–∏–ø—Ñ–∞:**  \n",
        "   –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤–∞ –æ—Ç –µ–≥–æ —Ä–∞–Ω–≥–∞ –≤ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–º –º–∞—Å—à—Ç–∞–±–µ. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ, –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é (–ø—Ä—è–º–∞—è –ª–∏–Ω–∏—è = –∑–∞–∫–æ–Ω –¶–∏–ø—Ñ–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è).\n",
        "\n",
        "2. **–í–ª–∏—è–Ω–∏–µ –Ω–∞ downstream-–∑–∞–¥–∞—á—É:**  \n",
        "   –û—Ü–µ–Ω–∏—Ç–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞ –∑–∞–¥–∞—á–µ **–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π** (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ —Ç–µ–º–∞–º: –ø–æ–ª–∏—Ç–∏–∫–∞, —Å–ø–æ—Ä—Ç, —ç–∫–æ–Ω–æ–º–∏–∫–∞). –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å (LogReg, SVM) –∏–ª–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä.\n",
        "\n",
        "3. **–£–≥–ª—É–±–ª–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:**  \n",
        "   –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏:\n",
        "   - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤ (–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞)\n",
        "   - –û–±–ª–∞–∫–æ —Å–ª–æ–≤ (WordCloud) –¥–ª—è —Ç–æ–ø-100 —Ç–æ–∫–µ–Ω–æ–≤\n",
        "   - Heatmap —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏\n",
        "\n",
        "\n",
        "## üìÑ 5. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç—á–µ—Ç—É\n",
        "\n",
        "–û—Ç—á—ë—Ç –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç–µ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:\n",
        "\n",
        "1. –¢–∏—Ç—É–ª—å–Ω—ã–π –ª–∏—Å—Ç (–§–ò–û, –≥—Ä—É–ø–ø–∞, –¥–∞—Ç–∞, –ø–æ–¥–ø–∏—Å—å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è)\n",
        "2. –ü–æ—Å—Ç–∞–Ω–æ–≤–∫—É –∑–∞–¥–∞—á–∏ –∏ —Ü–µ–ª–∏\n",
        "3. –û–ø–∏—Å–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤\n",
        "4. –ê–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—Å —Ç–∞–±–ª–∏—Ü–∞–º–∏, –≥—Ä–∞—Ñ–∏–∫–∞–º–∏, —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏)\n",
        "5. –°—Å—ã–ª–∫–∏ –Ω–∞:\n",
        "   - –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ (GitHub/GitLab)\n",
        "   - –†–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–µ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (Hugging Face Space / Render / Railway)\n",
        "   - –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Hugging Face Hub)\n",
        "6. –í—ã–≤–æ–¥—ã –ø–æ —Ä–∞–±–æ—Ç–µ\n",
        "7. **–†–µ—Ñ–ª–µ–∫—Å–∏—è:** –ß—Ç–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ª—É—á—à–µ –≤—Å–µ–≥–æ? –° –∫–∞–∫–∏–º–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å? –ß—Ç–æ –±—ã —Å–¥–µ–ª–∞–ª–∏ –∏–Ω–∞—á–µ?\n",
        "\n",
        "\n",
        "\n",
        "## üìä 6. –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏\n",
        "\n",
        "| –û—Ü–µ–Ω–∫–∞             | –ö—Ä–∏—Ç–µ—Ä–∏–∏ |\n",
        "|--------------------|----------|\n",
        "| **–û—Ç–ª–∏—á–Ω–æ (5)**    | –ü–æ–ª–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤, –≤–∫–ª—é—á–∞—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è. –ù–∞–ª–∏—á–∏–µ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ –≤–µ–±-API –∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ Hugging Face. –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, —Ä–µ—Ñ–ª–µ–∫—Å–∏—è. **+ –ø—É–±–ª–∏—á–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ Space/Gradio App.** |\n",
        "| **–•–æ—Ä–æ—à–æ (4)**     | –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ (1‚Äì7). –ù–∞–ª–∏—á–∏–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞. –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞. |\n",
        "| **–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ (3)** | –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–∞–ø–æ–≤ 1‚Äì4. –ù–∞–ª–∏—á–∏–µ –æ—Ç—á—ë—Ç–∞ —Å –±–∞–∑–æ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –∏ —Ç–∞–±–ª–∏—Ü–µ–π –º–µ—Ç—Ä–∏–∫. |\n",
        "| **–ù–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ (2)** | –≠—Ç–∞–ø—ã –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã, –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ—Ç—á—ë—Ç. |\n",
        "\n",
        "\n",
        "## üìñ 7. –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞\n",
        "\n",
        "1. Jurafsky, D., Martin, J. H. *Speech and Language Processing*. ‚Äî 3rd ed., 2021.  \n",
        "2. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫:  \n",
        "   - [Hugging Face Transformers](https://huggingface.co/docs/transformers/)  \n",
        "   - [spaCy](https://spacy.io/usage)  \n",
        "   - [NLTK](https://www.nltk.org/book/)  \n",
        "   - [SentencePiece](https://github.com/google/sentencepiece)  \n",
        "3. –°—Ç–∞—Ç—å–∏:  \n",
        "   - Sennrich, R., Haddow, B., Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units*.  \n",
        "   - Kudo, T. (2018). *Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates*.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_Bvpf09WoTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q razdel spacy nltk sentence_transformers tqdm pymorphy3 pymorphy3-dicts-ru\n",
        "!python -m spacy download ru_core_news_sm\n"
      ],
      "metadata": {
        "id": "d6fUcxoi4rJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/pymorphy2/pymorphy2.git\n",
        "!pip install -q razdel spacy nltk sentence_transformers tqdm\n",
        "!python -m spacy download ru_core_news_sm\n"
      ],
      "metadata": {
        "id": "a1SWXRgEO8U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –≠–¢–ê–ü 1. –°–±–æ—Ä –∫–æ—Ä–ø—É—Å–∞ (‚â• 50 000 —Å–ª–æ–≤)"
      ],
      "metadata": {
        "id": "gQohJ-knJYLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json, time\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from tqdm import tqdm\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; Bot/0.1)\"}\n",
        "\n",
        "def parse_lenta_article(url):\n",
        "    \"\"\"\n",
        "    –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏ —Å Lenta.ru\n",
        "    \"\"\"\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers, timeout=10)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "        title = soup.find('h1').get_text(strip=True) if soup.find('h1') else ''\n",
        "        article = soup.find('article') or soup\n",
        "        paragraphs = [p.get_text(\" \", strip=True) for p in article.find_all('p')]\n",
        "        text = \"\\n\".join(paragraphs)\n",
        "\n",
        "        return {\"url\": url, \"title\": title, \"text\": text, \"source\": \"lenta.ru\"}\n",
        "    except Exception:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "Ih__Nxh4Itmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_page = \"https://lenta.ru/rubrics/world/\"\n",
        "res = requests.get(start_page, headers=headers)\n",
        "soup = BeautifulSoup(res.text, 'lxml')\n",
        "\n",
        "# –°—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏\n",
        "links = []\n",
        "for a in soup.select('a[href^=\"/news/\"], a[href^=\"/articles/\"]'):\n",
        "    href = a.get('href')\n",
        "    if href and href.startswith('/'):\n",
        "        links.append(urljoin(\"https://lenta.ru\", href.split('?')[0]))\n",
        "\n",
        "# –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
        "links = list(dict.fromkeys(links))[:250]\n",
        "\n",
        "# –°–±–æ—Ä —Å—Ç–∞—Ç–µ–π\n",
        "articles = []\n",
        "for link in tqdm(links):\n",
        "    art = parse_lenta_article(link)\n",
        "    if art and len(art['text']) > 200:\n",
        "        articles.append(art)\n",
        "    time.sleep(0.2)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "with open('corpus.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for a in articles:\n",
        "        f.write(json.dumps(a, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"‚úÖ –°–æ–±—Ä–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}\")\n"
      ],
      "metadata": {
        "id": "8-3hKdPCI5Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"corpus.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    articles = [json.loads(line) for line in f]\n",
        "\n",
        "word_count = sum(len(a[\"text\"].split()) for a in articles)\n",
        "print(f\"üìä –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}\")\n",
        "print(f\"üìù –í—Å–µ–≥–æ —Å–ª–æ–≤: {word_count}\")\n"
      ],
      "metadata": {
        "id": "ZY6lOCzfJpnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json, time\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from tqdm import tqdm\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; Bot/0.1)\"}\n",
        "\n",
        "def parse_lenta_article(url):\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers, timeout=10)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text, 'lxml')\n",
        "        title = soup.find('h1').get_text(strip=True) if soup.find('h1') else ''\n",
        "        article = soup.find('article') or soup\n",
        "        paragraphs = [p.get_text(\" \", strip=True) for p in article.find_all('p')]\n",
        "        text = \"\\n\".join(paragraphs)\n",
        "        return {\"url\": url, \"title\": title, \"text\": text, \"source\": \"lenta.ru\"}\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# –ù–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª Lenta.ru ‚Äî –†–æ—Å—Å–∏—è\n",
        "start_page = \"https://lenta.ru/rubrics/russia/\"\n",
        "res = requests.get(start_page, headers=headers)\n",
        "soup = BeautifulSoup(res.text, 'lxml')\n",
        "\n",
        "links = []\n",
        "for a in soup.select('a[href^=\"/news/\"], a[href^=\"/articles/\"]'):\n",
        "    href = a.get('href')\n",
        "    if href and href.startswith('/'):\n",
        "        links.append(urljoin(\"https://lenta.ru\", href.split('?')[0]))\n",
        "\n",
        "links = list(dict.fromkeys(links))[:150]\n",
        "\n",
        "# –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫ —Å—Ç–∞—Ä–æ–º—É –∫–æ—Ä–ø—É—Å—É\n",
        "articles = []\n",
        "for link in tqdm(links):\n",
        "    art = parse_lenta_article(link)\n",
        "    if art and len(art['text']) > 200:\n",
        "        articles.append(art)\n",
        "    time.sleep(0.2)\n",
        "\n",
        "print(f\"‚úÖ –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π —Å–æ–±—Ä–∞–Ω–æ: {len(articles)}\")\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –∫–æ—Ä–ø—É—Å—É\n",
        "with open(\"corpus.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "    for a in articles:\n",
        "        f.write(json.dumps(a, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"üìÅ –ù–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ corpus.jsonl\")\n"
      ],
      "metadata": {
        "id": "L24wNL6NJ7dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"corpus.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    articles = [json.loads(line) for line in f]\n",
        "\n",
        "word_count = sum(len(a[\"text\"].split()) for a in articles)\n",
        "print(f\"üìä –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}\")\n",
        "print(f\"üìù –í—Å–µ–≥–æ —Å–ª–æ–≤: {word_count}\")\n"
      ],
      "metadata": {
        "id": "yzL340KAKBnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –≠–¢–ê–ü 2. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
      ],
      "metadata": {
        "id": "KaT5It8KJ5E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ text_cleaner.py\n",
        "%%writefile text_cleaner.py\n",
        "import re\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = set(stopwords.words(\"russian\"))\n",
        "\n",
        "def clean_text(text, remove_stopwords=True, lower=True):\n",
        "    \"\"\"\n",
        "    –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º HTML-—Ç–µ–≥–∏\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "\n",
        "    # –ó–∞–º–µ–Ω—è–µ–º URL –∏ email –Ω–∞ —Ç–æ–∫–µ–Ω—ã\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
        "\n",
        "    # –ó–∞–º–µ–Ω—è–µ–º —á–∏—Å–ª–∞\n",
        "    text = re.sub(r'\\b\\d+([.,]\\d+)?\\b', '<NUM>', text)\n",
        "\n",
        "    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    text = re.sub(r'[^–∞-—è–ê-–Ø—ë–Å0-9<>\\s]', ' ', text)\n",
        "\n",
        "    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–¥–Ω–æ–º—É –ø—Ä–æ–±–µ–ª—É\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    if lower:\n",
        "        text = text.lower()\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
        "    if remove_stopwords:\n",
        "        words = text.split()\n",
        "        words = [w for w in words if w not in russian_stopwords]\n",
        "        text = ' '.join(words)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def clean_corpus(input_file=\"corpus.jsonl\", output_file=\"corpus_clean.jsonl\"):\n",
        "    \"\"\"\n",
        "    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ—á–∏—Å—Ç–∫—É –∫–æ –≤—Å–µ–º —Å—Ç–∞—Ç—å—è–º –∫–æ—Ä–ø—É—Å–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n",
        "    \"\"\"\n",
        "    cleaned_articles = []\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        articles = [json.loads(line) for line in f]\n",
        "\n",
        "    for a in articles:\n",
        "        a_clean = a.copy()\n",
        "        a_clean[\"text\"] = clean_text(a[\"text\"])\n",
        "        cleaned_articles.append(a_clean)\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for a in cleaned_articles:\n",
        "            f.write(json.dumps(a, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ –û—á–∏—â–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_file}\")\n"
      ],
      "metadata": {
        "id": "5oQzmI_JKy0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from text_cleaner import clean_corpus\n",
        "\n",
        "# –û—á–∏—â–∞–µ–º –∫–æ—Ä–ø—É—Å\n",
        "clean_corpus(\"corpus.jsonl\", \"corpus_clean.jsonl\")\n"
      ],
      "metadata": {
        "id": "iBqrbxgeK5QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"corpus_clean.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    articles = [json.loads(line) for line in f]\n",
        "\n",
        "word_count = sum(len(a[\"text\"].split()) for a in articles)\n",
        "print(f\"üìä –û—á–∏—â–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(articles)}\")\n",
        "print(f\"üßπ –í—Å–µ–≥–æ —Å–ª–æ–≤ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {word_count}\")\n"
      ],
      "metadata": {
        "id": "IoBqL0KrK-mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –≠–¢–ê–ü 3. –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞)"
      ],
      "metadata": {
        "id": "xlYQCAo7LMeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile universal_preprocessor.py\n",
        "import re\n",
        "\n",
        "class UniversalPreprocessor:\n",
        "    \"\"\"\n",
        "    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Ä–µ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π.\n",
        "    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞–º–µ–Ω—É URL, email, —á–∏—Å–µ–ª, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –æ—á–∏—Å—Ç–∫—É –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lower=True, remove_extra_spaces=True, replace_special=True):\n",
        "        self.lower = lower\n",
        "        self.remove_extra_spaces = remove_extra_spaces\n",
        "        self.replace_special = replace_special\n",
        "\n",
        "        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ä–∞—Å–∫—Ä—ã—Ç–∏—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π\n",
        "        self.abbreviations = {\n",
        "            r\"\\b—Ç\\.–µ\\.\": \"—Ç–æ –µ—Å—Ç—å\",\n",
        "            r\"\\b–≥\\.\": \"–≥–æ–¥\",\n",
        "            r\"\\b—É–ª\\.\": \"—É–ª–∏—Ü–∞\",\n",
        "            r\"\\b–¥\\.\": \"–¥–æ–º\",\n",
        "            r\"\\b—Ä–∏—Å\\.\": \"—Ä–∏—Å—É–Ω–æ–∫\",\n",
        "            r\"\\b—Å—Ç—Ä\\.\": \"—Å—Ç—Ä–∞–Ω–∏—Ü–∞\",\n",
        "            r\"\\b–∏–º\\.\": \"–∏–º–µ–Ω–∏\",\n",
        "            r\"\\b–º–∏–Ω\\.\": \"–º–∏–Ω—É—Ç–∞\",\n",
        "            r\"\\b—Å–µ–∫\\.\": \"—Å–µ–∫—É–Ω–¥–∞\",\n",
        "        }\n",
        "\n",
        "    def replace_tokens(self, text):\n",
        "        \"\"\"–ó–∞–º–µ–Ω—è–µ—Ç URL, email, —á–∏—Å–ª–∞ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã\"\"\"\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text)\n",
        "        text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
        "        text = re.sub(r'\\b\\d+([.,]\\d+)?\\b', '<NUM>', text)\n",
        "        return text\n",
        "\n",
        "    def expand_abbreviations(self, text):\n",
        "        \"\"\"–†–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –æ–±—â–µ—É–ø–æ—Ç—Ä–µ–±–∏–º—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\"\"\"\n",
        "        for pattern, replacement in self.abbreviations.items():\n",
        "            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "\n",
        "    def normalize_punctuation(self, text):\n",
        "        \"\"\"–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ø—Ä–æ–±–µ–ª—ã\"\"\"\n",
        "        text = re.sub(r'[‚Äú‚Äù¬´¬ª]', '\"', text)\n",
        "        text = re.sub(r'[‚Äì‚Äî]', '-', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "\n",
        "        if self.replace_special:\n",
        "            text = self.replace_tokens(text)\n",
        "\n",
        "        text = self.expand_abbreviations(text)\n",
        "        text = self.normalize_punctuation(text)\n",
        "\n",
        "        if self.remove_extra_spaces:\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "V2oVYPq1LLo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from universal_preprocessor import UniversalPreprocessor\n",
        "\n",
        "prep = UniversalPreprocessor()\n",
        "\n",
        "sample = \"–í 2025 –≥. –Ω–∞ —Å–∞–π—Ç–µ https://lenta.ru –ø–æ—è–≤–∏–ª–æ—Å—å 10 –Ω–æ–≤–æ—Å—Ç–µ–π, —Ç.–µ. <EMAIL> –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –∞–≤—Ç–æ—Ä–∞–º!\"\n",
        "print(\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\")\n",
        "print(sample)\n",
        "print(\"\\n–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
        "print(prep.preprocess(sample))\n"
      ],
      "metadata": {
        "id": "aRctqVVZLmTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –≠—Ç–∞–ø 4. –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"
      ],
      "metadata": {
        "id": "5BXVCSnFMYrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open(\"corpus_clean.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    articles = [json.loads(line) for line in f]\n",
        "\n",
        "texts = [a['text'] for a in articles]\n",
        "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(texts)}\")\n"
      ],
      "metadata": {
        "id": "EO4G2t6FMyU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ò–º–ø–æ—Ä—Ç—ã\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import re, nltk, json\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "import spacy\n",
        "from universal_preprocessor import UniversalPreprocessor\n",
        "\n",
        "# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "morph = MorphAnalyzer()\n",
        "snow = SnowballStemmer(\"russian\")\n",
        "prep = UniversalPreprocessor()\n",
        "\n",
        "# --- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ---\n",
        "def tok_naive(text):\n",
        "    return text.split()\n",
        "\n",
        "TOKEN_RE = re.compile(r\"[A-Za-z–ê-–Ø–∞-—è–Å—ë0-9]+(?:[-'][A-Za-z–ê-–Ø–∞-—è–Å—ë0-9]+)*\")\n",
        "def tok_regex(text):\n",
        "    return TOKEN_RE.findall(text)\n",
        "\n",
        "def tok_nltk(text):\n",
        "    return word_tokenize(text, language='russian')\n",
        "\n",
        "def tok_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [t.text for t in doc]\n",
        "\n",
        "def tok_razdel(text):\n",
        "    # –ú–∏–Ω–∏-—Ñ–∏–∫—Å: —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Å—á–∏—Ç–∞—é—Ç—Å—è –æ–¥–Ω–∏–º —Ç–æ–∫–µ–Ω–æ–º\n",
        "    text_safe = text.replace(\"<NUM>\", \"NUM_\").replace(\"<URL>\", \"URL_\").replace(\"<EMAIL>\", \"EMAIL_\")\n",
        "    return [t.text for t in razdel_tokenize(text_safe)]\n",
        "\n",
        "# --- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ---\n",
        "def stem_snowball(tokens):\n",
        "    return [snow.stem(t) for t in tokens]\n",
        "\n",
        "def lemmatize_pymorphy(tokens):\n",
        "    lemmas = []\n",
        "    for t in tokens:\n",
        "        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "        if t in [\"NUM_\", \"URL_\", \"EMAIL_\"]:\n",
        "            lemmas.append(t)\n",
        "            continue\n",
        "\n",
        "        t_clean = t.lower().strip(\".!,?:;/\\\\'\\\"-\")\n",
        "        if not t_clean or not re.search(r\"[–∞-—è—ë]\", t_clean):\n",
        "            continue  # –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–∫–µ–Ω—ã –±–µ–∑ —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤\n",
        "\n",
        "        try:\n",
        "            p = morph.parse(t_clean)[0]\n",
        "            if p.normal_form:\n",
        "                lemmas.append(p.normal_form)\n",
        "        except Exception:\n",
        "            lemmas.append(t_clean)\n",
        "    return lemmas\n",
        "\n",
        "\n",
        "def lemmatize_spacy(tokens):\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    return [t.lemma_ for t in doc if t.text.strip()]\n",
        "\n",
        "# --- –°–ª–æ–≤–∞—Ä–∏ –º–µ—Ç–æ–¥–æ–≤ ---\n",
        "TOKENIZERS = {\n",
        "    'naive': tok_naive,\n",
        "    'regex': tok_regex,\n",
        "    'nltk': tok_nltk,\n",
        "    'spacy': tok_spacy,\n",
        "    'razdel': tok_razdel\n",
        "}\n",
        "\n",
        "NORMALIZERS = {\n",
        "    'snowball_stem': stem_snowball,\n",
        "    'pymorphy_lemma': lemmatize_pymorphy,\n",
        "    'spacy_lemma': lemmatize_spacy\n",
        "}\n",
        "\n",
        "# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ ---\n",
        "sample_text = \"–í 2025 –≥. –Ω–∞ —Å–∞–π—Ç–µ https://lenta.ru –ø–æ—è–≤–∏–ª–æ—Å—å 10 –Ω–æ–≤–æ—Å—Ç–µ–π, —Ç.–µ. –ø–∏—Å—å–º–æ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –∞–≤—Ç–æ—Ä–∞–º!\"\n",
        "\n",
        "# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "text_preprocessed = prep.preprocess(sample_text)\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
        "tokens = tok_razdel(text_preprocessed)\n",
        "lemmas = lemmatize_pymorphy(tokens)\n",
        "\n",
        "# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "print(\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\", sample_text)\n",
        "print(\"–ü–æ—Å–ª–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞:\", text_preprocessed)\n",
        "print(\"–¢–æ–∫–µ–Ω—ã (razdel –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ):\", tokens)\n",
        "print(\"–õ–µ–º–º—ã (pymorphy3):\", lemmas)\n"
      ],
      "metadata": {
        "id": "uCq9Xk2PNeaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')        # –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å\n",
        "nltk.download('averaged_perceptron_tagger')  # –µ—Å–ª–∏ –±—É–¥—É—Ç POS\n"
      ],
      "metadata": {
        "id": "PkfcpH1vDJ_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# --- –ò–º–ø–æ—Ä—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ ---\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import spacy\n",
        "from text_cleaner import clean_text\n",
        "from universal_preprocessor import UniversalPreprocessor\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---\n",
        "prep = UniversalPreprocessor()\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "morph = MorphAnalyzer()\n",
        "snow = SnowballStemmer(\"russian\")\n",
        "\n",
        "# --- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ---\n",
        "def tok_naive(text):\n",
        "    return text.split()\n",
        "\n",
        "TOKEN_RE = re.compile(r\"[A-Za-z–ê-–Ø–∞-—è–Å—ë0-9]+(?:[-'][A-Za-z–ê-–Ø–∞-—è–Å—ë0-9]+)*\")\n",
        "def tok_regex(text):\n",
        "    return TOKEN_RE.findall(text)\n",
        "\n",
        "def tok_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [t.text for t in doc]\n",
        "\n",
        "def tok_razdel(text):\n",
        "    text = re.sub(r'(<NUM>|<URL>|<EMAIL>)', r' \\1 ', text)\n",
        "    return [t.text for t in razdel_tokenize(text)]\n",
        "\n",
        "TOKENIZERS = {\n",
        "    'naive': tok_naive,\n",
        "    'regex': tok_regex,\n",
        "    'spacy': tok_spacy,\n",
        "    'razdel': tok_razdel\n",
        "}\n",
        "\n",
        "# --- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ---\n",
        "def stem_snowball(tokens):\n",
        "    return [snow.stem(t) for t in tokens]\n",
        "\n",
        "def lemmatize_pymorphy(tokens):\n",
        "    lemmas = []\n",
        "    for t in tokens:\n",
        "        t_clean = t.lower().strip(\".!,?:;/\\\\'\\\"-\")\n",
        "        if not t_clean or not re.search(r\"[–∞-—è—ë]\", t_clean):\n",
        "            continue\n",
        "        try:\n",
        "            p = morph.parse(t_clean)[0]\n",
        "            lemmas.append(p.normal_form)\n",
        "        except Exception:\n",
        "            lemmas.append(t_clean)\n",
        "    return lemmas\n",
        "\n",
        "def lemmatize_spacy(tokens):\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    return [t.lemma_ for t in doc if t.text.strip()]\n",
        "\n",
        "NORMALIZERS = {\n",
        "    'snowball_stem': stem_snowball,\n",
        "    'pymorphy_lemma': lemmatize_pymorphy,\n",
        "    'spacy_lemma': lemmatize_spacy\n",
        "}\n",
        "\n",
        "# --- –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ—Ä–ø—É—Å ---\n",
        "with open(\"corpus_clean.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    articles = [json.loads(line) for line in f]\n",
        "\n",
        "texts = [a[\"text\"] for a in articles]\n",
        "print(f\"–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(texts)}\")\n",
        "\n",
        "# --- –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ ---\n",
        "results = []\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "for tok_name, tok_func in TOKENIZERS.items():\n",
        "    start_time = time.time()\n",
        "    all_tokens = []\n",
        "    for text in texts:\n",
        "        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ (—Ä–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π, —á–∏—Å–ª–∞, URL)\n",
        "        text_preprocessed = prep.preprocess(text)\n",
        "        tokens = tok_func(text_preprocessed)\n",
        "        all_tokens.extend(tokens)\n",
        "    tok_time = time.time() - start_time\n",
        "    vocab_size = len(set(all_tokens))\n",
        "    results.append({\n",
        "        \"method\": tok_name,\n",
        "        \"type\": \"tokenizer\",\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"time_per_1000\": tok_time / len(texts) * 1000,\n",
        "        \"OOV_rate\": None\n",
        "    })\n",
        "\n",
        "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "for norm_name, norm_func in NORMALIZERS.items():\n",
        "    start_time = time.time()\n",
        "    all_lemmas = []\n",
        "    for text in texts:\n",
        "        text_preprocessed = prep.preprocess(text)\n",
        "        tokens = tok_razdel(text_preprocessed)\n",
        "        lemmas = norm_func(tokens)\n",
        "        all_lemmas.extend(lemmas)\n",
        "    norm_time = time.time() - start_time\n",
        "    vocab_size = len(set(all_lemmas))\n",
        "    results.append({\n",
        "        \"method\": norm_name,\n",
        "        \"type\": \"normalizer\",\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"time_per_1000\": norm_time / len(texts) * 1000,\n",
        "        \"OOV_rate\": None\n",
        "    })\n",
        "\n",
        "# --- –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ---\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"tokenization_metrics.csv\", index=False)\n",
        "print(\"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ tokenization_metrics.csv\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "wlchWNPiC-Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# —ç—Ç–∞–ø 5 ‚Äî –æ–±—É—á–µ–Ω–∏–µ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
      ],
      "metadata": {
        "id": "0ZJmjRh9Gs5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "from tokenizers.normalizers import Lowercase, Sequence, NFD, StripAccents\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç –∫–æ—Ä–ø—É—Å–∞ ---\n",
        "with open(\"corpus_clean.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    articles = [json.loads(line) for line in f]\n",
        "\n",
        "texts = [a[\"text\"] for a in articles]\n",
        "print(f\"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Å—Ç–∞—Ç–µ–π\")\n",
        "\n",
        "# --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ plain txt –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ---\n",
        "with open(\"corpus_for_tokenizer.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in texts:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è ---\n",
        "vocab_sizes = [8000, 16000, 32000]  # –ø—Ä–∏–º–µ—Ä —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ —Å–ª–æ–≤–∞—Ä—è\n",
        "min_frequency = 2  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Ç–æ–∫–µ–Ω–∞\n",
        "\n",
        "# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ ---\n",
        "def train_tokenizer(model_type=\"BPE\", vocab_size=16000, file_path=\"corpus_for_tokenizer.txt\"):\n",
        "    if model_type == \"BPE\":\n",
        "        tokenizer = Tokenizer(models.BPE())\n",
        "    elif model_type == \"WordPiece\":\n",
        "        tokenizer = Tokenizer(models.WordPiece())\n",
        "    elif model_type == \"Unigram\":\n",
        "        tokenizer = Tokenizer(models.Unigram())\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model type\")\n",
        "\n",
        "    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: lowercase + NFD\n",
        "    tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
        "\n",
        "    # –ü—Ä–µ-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "    # –¢—Ä–µ–Ω–µ—Ä\n",
        "    if model_type == \"BPE\":\n",
        "        trainer = trainers.BpeTrainer(vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=[\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", \"<MASK>\"])\n",
        "    elif model_type == \"WordPiece\":\n",
        "        trainer = trainers.WordPieceTrainer(vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=[\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", \"<MASK>\"])\n",
        "    elif model_type == \"Unigram\":\n",
        "        trainer = trainers.UnigramTrainer(vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=[\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", \"<MASK>\"])\n",
        "\n",
        "    # –û–±—É—á–µ–Ω–∏–µ\n",
        "    tokenizer.train([file_path], trainer=trainer)\n",
        "    return tokenizer\n",
        "\n",
        "# --- –ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è BPE –Ω–∞ 16k —Å–ª–æ–≤ ---\n",
        "bpe_tokenizer = train_tokenizer(\"BPE\", 16000)\n",
        "bpe_tokenizer.save(\"bpe_16k.json\")\n",
        "\n",
        "# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ---\n",
        "sample_text = \"–í 2025 –≥. –Ω–∞ —Å–∞–π—Ç–µ https://lenta.ru –ø–æ—è–≤–∏–ª–æ—Å—å 10 –Ω–æ–≤–æ—Å—Ç–µ–π, —Ç.–µ. –ø–∏—Å—å–º–æ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –∞–≤—Ç–æ—Ä–∞–º!\"\n",
        "output = bpe_tokenizer.encode(sample_text)\n",
        "print(\"–¢–æ–∫–µ–Ω—ã BPE:\", output.tokens)\n",
        "print(\"–ò–Ω–¥–µ–∫—Å—ã —Ç–æ–∫–µ–Ω–æ–≤:\", output.ids)\n"
      ],
      "metadata": {
        "id": "fpmP-XgIGwUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "!pip install -q tokenizers sentencepiece tqdm\n",
        "\n",
        "import json\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "from tokenizers.processors import BertProcessing\n",
        "from tqdm import tqdm\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ—Ä–ø—É—Å\n",
        "with open(\"corpus_clean.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    articles = [json.loads(line) for line in f]\n",
        "texts = [a[\"text\"] for a in articles]\n",
        "print(f\"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Å—Ç–∞—Ç–µ–π\")\n",
        "\n",
        "# --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ ---\n",
        "with open(\"corpus_for_tokenizers.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in texts:\n",
        "        f.write(line + \"\\n\")\n",
        "from tokenizers import trainers as t_trainers\n",
        "\n",
        "def train_subword_model(model_type=\"BPE\", vocab_size=16000, min_frequency=2, corpus_file=\"corpus_for_tokenizers.txt\"):\n",
        "    if model_type == \"BPE\":\n",
        "        tokenizer = Tokenizer(models.BPE())\n",
        "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "        trainer = t_trainers.BpeTrainer(\n",
        "            vocab_size=vocab_size,\n",
        "            min_frequency=min_frequency,\n",
        "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        "        )\n",
        "    elif model_type == \"WordPiece\":\n",
        "        tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "        trainer = t_trainers.WordPieceTrainer(\n",
        "            vocab_size=vocab_size,\n",
        "            min_frequency=min_frequency,\n",
        "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        "        )\n",
        "    elif model_type == \"Unigram\":\n",
        "        from tokenizers import models as t_models\n",
        "        tokenizer = Tokenizer(t_models.Unigram())\n",
        "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "        trainer = t_trainers.UnigramTrainer(\n",
        "            vocab_size=vocab_size,\n",
        "            min_frequency=min_frequency,\n",
        "            special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏\")\n",
        "\n",
        "    tokenizer.train([corpus_file], trainer)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "# --- –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ ---\n",
        "bpe_tokenizer = train_subword_model(\"BPE\", vocab_size=16000)\n",
        "wp_tokenizer = train_subword_model(\"WordPiece\", vocab_size=16000)\n",
        "unigram_tokenizer = train_subword_model(\"Unigram\", vocab_size=16000)\n",
        "\n",
        "# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ –º–µ—Ç—Ä–∏–∫ ---\n",
        "def subword_metrics(tokenizer, texts):\n",
        "    total_words = 0\n",
        "    total_tokens = 0\n",
        "    frag_count = 0\n",
        "\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        total_words += len(words)\n",
        "        encoding = tokenizer.encode(text)\n",
        "        tokens = encoding.tokens\n",
        "        total_tokens += len(tokens)\n",
        "\n",
        "        # –°—á–∏—Ç–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—é: —Å–ª–æ–≤–æ —Ä–∞–∑–±–∏—Ç–æ –Ω–∞ >1 —Ç–æ–∫–µ–Ω–∞\n",
        "        for w in words:\n",
        "            enc = tokenizer.encode(w)\n",
        "            if len(enc.tokens) > 1:\n",
        "                frag_count += 1\n",
        "\n",
        "    compression_ratio = total_words / total_tokens\n",
        "    fragmentation_rate = frag_count / total_words * 100\n",
        "    return {\n",
        "        \"total_words\": total_words,\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"compression_ratio\": compression_ratio,\n",
        "        \"fragmentation_rate_%\": fragmentation_rate\n",
        "    }\n",
        "\n",
        "# --- –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ ---\n",
        "bpe_metrics = subword_metrics(bpe_tokenizer, texts)\n",
        "wp_metrics = subword_metrics(wp_tokenizer, texts)\n",
        "unigram_metrics = subword_metrics(unigram_tokenizer, texts)\n",
        "\n",
        "print(\"üìä –ú–µ—Ç—Ä–∏–∫–∏ BPE:\", bpe_metrics)\n",
        "print(\"üìä –ú–µ—Ç—Ä–∏–∫–∏ WordPiece:\", wp_metrics)\n",
        "print(\"üìä –ú–µ—Ç—Ä–∏–∫–∏ Unigram:\", unigram_metrics)\n"
      ],
      "metadata": {
        "id": "J6kSbmsXHvo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, WordPiece, Unigram\n",
        "from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import json\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ—Ä–ø—É—Å\n",
        "with open(\"corpus_clean.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texts = [json.loads(line)[\"text\"] for line in f]\n",
        "\n",
        "print(f\"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Å—Ç–∞—Ç–µ–π\")\n",
        "\n",
        "# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –º–µ—Ç—Ä–∏–∫ ---\n",
        "def compute_subword_metrics(tokenizer: Tokenizer, texts):\n",
        "    total_words = sum(len(text.split()) for text in texts)\n",
        "    total_tokens = 0\n",
        "    fragmented = 0\n",
        "    reconstruction_acc = 0\n",
        "    n_samples = min(100, len(texts))  # –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–µ—Ä–≤—ã—Ö 100 —Ç–µ–∫—Å—Ç–∞—Ö –¥–ª—è reconstruction\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        enc = tokenizer.encode(text)\n",
        "        total_tokens += len(enc.ids)\n",
        "        fragmented += sum(1 for word_tokens in [tokenizer.encode(w).ids for w in text.split()] if len(word_tokens) > 1)\n",
        "\n",
        "        if i < n_samples:\n",
        "            recon = tokenizer.decode(enc.ids)\n",
        "            # —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤\n",
        "            matches = sum(a==b for a,b in zip(text, recon))\n",
        "            reconstruction_acc += matches / len(text)\n",
        "\n",
        "    return {\n",
        "        \"total_words\": total_words,\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"compression_ratio\": total_words / total_tokens,\n",
        "        \"fragmentation_rate_%\": fragmented / total_words * 100,\n",
        "        \"reconstruction_accuracy_%\": reconstruction_acc / n_samples * 100\n",
        "    }\n",
        "\n",
        "# --- –ü—Ä–∏–º–µ—Ä: BPE ---\n",
        "bpe_tokenizer = Tokenizer(BPE())\n",
        "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer_bpe = BpeTrainer(vocab_size=16000, min_frequency=2, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "bpe_tokenizer.train_from_iterator(texts, trainer_bpe)\n",
        "metrics_bpe = compute_subword_metrics(bpe_tokenizer, texts)\n",
        "print(\"üìä –ú–µ—Ç—Ä–∏–∫–∏ BPE:\", metrics_bpe)\n",
        "\n",
        "# --- WordPiece ---\n",
        "wp_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "wp_tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer_wp = WordPieceTrainer(vocab_size=16000, min_frequency=2, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "wp_tokenizer.train_from_iterator(texts, trainer_wp)\n",
        "metrics_wp = compute_subword_metrics(wp_tokenizer, texts)\n",
        "print(\"üìä –ú–µ—Ç—Ä–∏–∫–∏ WordPiece:\", metrics_wp)\n",
        "\n",
        "# --- Unigram ---\n",
        "unigram_tokenizer = Tokenizer(Unigram())\n",
        "unigram_tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer_unigram = UnigramTrainer(vocab_size=16000, min_frequency=2, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "unigram_tokenizer.train_from_iterator(texts, trainer_unigram)\n",
        "metrics_unigram = compute_subword_metrics(unigram_tokenizer, texts)\n",
        "print(\"üìä –ú–µ—Ç—Ä–∏–∫–∏ Unigram:\", metrics_unigram)\n"
      ],
      "metadata": {
        "id": "wjDih8N6K6du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –≠—Ç–∞–ø 6. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"
      ],
      "metadata": {
        "id": "Z6PZzHt3RhxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok razdel matplotlib seaborn pandas\n"
      ],
      "metadata": {
        "id": "dTCelYwpcbAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"33rrvngq55Ym4ttNvRgvHJi6I9z_3uyg5NWWBepyuEtYCYiey\")\n"
      ],
      "metadata": {
        "id": "dmQBY0uXig5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "st.title(\"üß† –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞\")\n",
        "st.write(\"–í—ã–±–µ—Ä–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\")\n",
        "\n",
        "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---\n",
        "uploaded_file = st.file_uploader(\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–∞–º–∏ (.csv –∏–ª–∏ .jsonl)\")\n",
        "if uploaded_file:\n",
        "    if uploaded_file.name.endswith(\".csv\"):\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "    elif uploaded_file.name.endswith(\".jsonl\"):\n",
        "        df = pd.read_json(uploaded_file, lines=True)\n",
        "    texts = df['text'].tolist()\n",
        "else:\n",
        "    st.info(\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä\")\n",
        "    texts = [\"–í 2025 –≥. –Ω–∞ —Å–∞–π—Ç–µ https://lenta.ru –ø–æ—è–≤–∏–ª–æ—Å—å 10 –Ω–æ–≤–æ—Å—Ç–µ–π, —Ç.–µ. –ø–∏—Å—å–º–æ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –∞–≤—Ç–æ—Ä–∞–º!\"]\n",
        "\n",
        "# --- –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ---\n",
        "method = st.selectbox(\"–ú–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\", [\"naive\", \"regex\", \"razdel\"])\n",
        "\n",
        "def tok_naive(text):\n",
        "    return text.split()\n",
        "\n",
        "def tok_regex(text):\n",
        "    import re\n",
        "    TOKEN_RE = re.compile(r\"[A-Za-z–ê-–Ø–∞-—è–Å—ë0-9]+(?:[-'][A-Za-z–ê-–Ø–∞-—è–Å—ë0-9]+)*\")\n",
        "    return TOKEN_RE.findall(text)\n",
        "\n",
        "def tok_razdel(text):\n",
        "    return [t.text for t in razdel_tokenize(text)]\n",
        "\n",
        "TOKENIZERS = {\"naive\": tok_naive, \"regex\": tok_regex, \"razdel\": tok_razdel}\n",
        "\n",
        "# --- –ê–Ω–∞–ª–∏–∑ ---\n",
        "all_tokens = []\n",
        "for text in texts:\n",
        "    all_tokens.extend(TOKENIZERS[method](text))\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫–∏\n",
        "token_lengths = [len(t) for t in all_tokens]\n",
        "freq = Counter(all_tokens)\n",
        "\n",
        "st.subheader(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤\")\n",
        "fig, ax = plt.subplots()\n",
        "sns.histplot(token_lengths, bins=20, ax=ax)\n",
        "st.pyplot(fig)\n",
        "\n",
        "st.subheader(\"–¢–æ–ø 20 —Ç–æ–∫–µ–Ω–æ–≤\")\n",
        "top_tokens = pd.DataFrame(freq.most_common(20), columns=[\"token\", \"count\"])\n",
        "st.bar_chart(top_tokens.set_index(\"token\"))\n",
        "\n",
        "st.write(\"–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤:\", len(all_tokens))\n",
        "st.write(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:\", len(set(all_tokens)))\n"
      ],
      "metadata": {
        "id": "UPYWmZ7uinHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()  # —É–±–∏–≤–∞–µ—Ç –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç—É–Ω–Ω–µ–ª–∏ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏\n"
      ],
      "metadata": {
        "id": "EJxxsY0Ei7WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(8501)\n",
        "print(\"üåê –ü–µ—Ä–µ–π–¥–∏ –ø–æ —Å—Å—ã–ª–∫–µ:\")\n",
        "print(public_url)\n"
      ],
      "metadata": {
        "id": "YdbS5lXDi88Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–º–µ—Ä–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "sample_texts = [\n",
        "    {\"text\": \"–í 2025 –≥. –Ω–∞ —Å–∞–π—Ç–µ https://lenta.ru –ø–æ—è–≤–∏–ª–æ—Å—å 10 –Ω–æ–≤–æ—Å—Ç–µ–π, —Ç.–µ. –ø–∏—Å—å–º–æ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –∞–≤—Ç–æ—Ä–∞–º!\"},\n",
        "    {\"text\": \"–°–µ–≥–æ–¥–Ω—è –ø–æ–≥–æ–¥–∞ –±—ã–ª–∞ —Å–æ–ª–Ω–µ—á–Ω–∞—è –∏ —Ç–µ–ø–ª–∞—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø—Ä–æ–≥—É–ª–∫—É –≤ –ø–∞—Ä–∫–µ.\"},\n",
        "    {\"text\": \"–ö–æ–º–ø–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –Ω–æ–≤—ã–π –ø—Ä–æ–¥—É–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—â–∞–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä—ã–Ω–æ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π.\"},\n",
        "    {\"text\": \"–ù–∞ —É—Ä–æ–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏ –¥–µ—Ç–∏ –∏–∑—É—á–∞–ª–∏ –æ—Å–Ω–æ–≤—ã Python –∏ —Å–æ–∑–¥–∞–≤–∞–ª–∏ –ø–µ—Ä–≤—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã.\"},\n",
        "    {\"text\": \"–í–µ—á–µ—Ä–æ–º –≤ –≥–æ—Ä–æ–¥–µ –ø—Ä–æ—à–µ–ª —Ñ–µ—Å—Ç–∏–≤–∞–ª—å –º—É–∑—ã–∫–∏ –∏ –∏—Å–∫—É—Å—Å—Ç–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏—à–ª–æ –º–Ω–æ–≥–æ –ª—é–¥–µ–π.\"}\n",
        "]\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JSONL\n",
        "with open(\"sample_texts.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in sample_texts:\n",
        "        f.write(f\"{line}\\n\")\n",
        "\n",
        "print(\"–§–∞–π–ª 'sample_texts.jsonl' –≥–æ—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏!\")"
      ],
      "metadata": {
        "id": "p6n7dC_pjlAg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}