{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daarlens/ProBook/blob/main/NLP/NLP-2025/%D0%9B%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D1%83%D0%BC_%E2%84%96_4_%D0%9B%D0%B5%D0%BD%D1%87%D0%B5%D0%BD%D0%BA%D0%BE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# üß™ **–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–π –ø—Ä–∞–∫—Ç–∏–∫—É–º ‚Ññ 4**  \n",
        "# **–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–∞—Ö**\n",
        "\n",
        "**–ö–∞—Ñ–µ–¥—Ä–∞:** –ö–∞—Ñ–µ–¥—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è  \n",
        "**–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞:** –û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞  \n",
        "**–£—Ä–æ–≤–µ–Ω—å:** –ú–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞, 2 –∫—É—Ä—Å  \n",
        "**–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å:** –ê—Ä–∞–±–æ–≤ –ú—É–ª–ª–æ—à–∞—Ä–∞—Ñ –ö—É—Ä–±–æ–Ω–æ–≤–∏—á  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ 1. –¶–µ–ª–∏ –∏ –∑–∞–¥–∞—á–∏ —Ä–∞–±–æ—Ç—ã\n",
        "\n",
        "### **–¶–µ–ª—å:**  \n",
        "–ü–æ–ª—É—á–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ **–≤—Å–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤**, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ. –ù–∞—É—á–∏—Ç—å—Å—è –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏.\n",
        "\n",
        "### **–ó–∞–¥–∞—á–∏:**  \n",
        "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—Ä–ø—É—Å –∏–∑ **–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã ‚Ññ1** (‚â•10 000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤).  \n",
        "2. –ü—Ä–∏–º–µ–Ω–∏—Ç—å **–º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏**, –≤–∫–ª—é—á–∞—è **—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é BPE-–º–æ–¥–µ–ª—å**, –æ–±—É—á–µ–Ω–Ω—É—é –≤ –õ–† ‚Ññ1.  \n",
        "3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–º–æ–¥–µ–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏**, –æ–±—É—á–µ–Ω–Ω—ã–µ –≤ **–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç–µ ‚Ññ2**:  \n",
        "   - Word2Vec (CBOW / Skip-Gram)  \n",
        "   - FastText (CBOW / Skip-Gram)  \n",
        "   - GloVe  \n",
        "4. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å **–≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏**:  \n",
        "   - **–¶–µ–Ω—Ç—Ä–æ–∏–¥–Ω—ã–µ**: k-Means, Mini-Batch k-Means, Spherical k-Means  \n",
        "   - **–ü–ª–æ—Ç–Ω–æ—Å—Ç–Ω—ã–µ**: DBSCAN, HDBSCAN  \n",
        "   - **–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ**: –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (single, complete, average, ward)  \n",
        "   - **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ**: Gaussian Mixture Models (GMM), Latent Dirichlet Allocation (LDA)  \n",
        "   - **–ì—Ä–∞—Ñ–æ–≤—ã–µ**: —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è  \n",
        "5. –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é **–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∏ –≤–Ω–µ—à–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫**.  \n",
        "6. –ü—Ä–æ–≤–µ—Å—Ç–∏ **–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é** –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.  \n",
        "7. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å **—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å** –∫ —à—É–º—É, –¥–∏—Å–±–∞–ª–∞–Ω—Å—É –∏ –æ–±—ä—ë–º—É –¥–∞–Ω–Ω—ã—Ö.  \n",
        "8. –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å **–≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö 2. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏\n",
        "\n",
        "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ ‚Äî –∑–∞–¥–∞—á–∞ **–æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è**, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –Ω–∞ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É **–±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏**. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è **—Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ, –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ –º–µ—Ç–æ–¥—ã**, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤ –∑—Ä–µ–ª—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ö (`scikit-learn`, `Gensim`, `hdbscan`).\n",
        "\n",
        "### **2.1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–∏–∑ –õ–† ‚Ññ1)**\n",
        "- Whitespace  \n",
        "- –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è  \n",
        "- **Byte Pair Encoding (BPE)** ‚Äî –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –õ–† ‚Ññ1 –º–æ–¥–µ–ª—å (—á–µ—Ä–µ–∑ `subword-nmt` –∏–ª–∏ –∞–Ω–∞–ª–æ–≥)\n",
        "\n",
        "### **2.2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (–∏–∑ –õ–† ‚Ññ2)**\n",
        "- **TF-IDF**, **BM25** ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞  \n",
        "- **Word2Vec**, **GloVe**, **FastText** ‚Äî —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –≤ –õ–† ‚Ññ2 –Ω–∞ —Ç–æ–º –∂–µ –∫–æ—Ä–ø—É—Å–µ  \n",
        "> –í–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ = —Å—Ä–µ–¥–Ω–µ–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º. –í—Å–µ –º–æ–¥–µ–ª–∏ ‚Äî **—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ**, –Ω–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –∏–∑–≤–Ω–µ.\n",
        "\n",
        "### **2.3. –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏**\n",
        "\n",
        "–í—Å–µ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ö:\n",
        "\n",
        "#### **–¶–µ–Ω—Ç—Ä–æ–∏–¥–Ω—ã–µ** (`scikit-learn`):\n",
        "- `KMeans` ‚Äî –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—É–º–º—É –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤  \n",
        "- `MiniBatchKMeans` ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö  \n",
        "- `SphericalKMeans` ‚Äî –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö (–∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö) –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤\n",
        "\n",
        "#### **–ü–ª–æ—Ç–Ω–æ—Å—Ç–Ω—ã–µ** (`scikit-learn`, `hdbscan`):\n",
        "- `DBSCAN` ‚Äî –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã, —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º  \n",
        "- `HDBSCAN` ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –≤–µ—Ä—Å–∏—è DBSCAN, **–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤**\n",
        "\n",
        "#### **–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ** (`scikit-learn`):\n",
        "- `AgglomerativeClustering` —Å:\n",
        "  - `linkage='ward'` (—Ç—Ä–µ–±—É–µ—Ç –µ–≤–∫–ª–∏–¥–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏)  \n",
        "  - `linkage='average'`, `'complete'`, `'single'` (—Ä–∞–±–æ—Ç–∞—é—Ç —Å `metric='cosine'`)\n",
        "\n",
        "#### **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ**:\n",
        "- `GaussianMixture` (`scikit-learn`) ‚Äî –º—è–≥–∫–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ç–æ—á–µ–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞–º  \n",
        "- `LdaModel` (`Gensim`) ‚Äî –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º–∞—è –∫–∞–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä –ø–æ —Ç–µ–º–∞–º\n",
        "\n",
        "#### **–ì—Ä–∞—Ñ–æ–≤—ã–µ** (`scikit-learn`):\n",
        "- `SpectralClustering` ‚Äî –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏\n",
        "\n",
        "> ‚ö†Ô∏è –í—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Å `numpy.ndarray`, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö **—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º–∏** –¥–ª—è –ª—é–±–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ 3. –ú–µ—Ç–æ–¥–∏–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
        "\n",
        "### **3.1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ—Ä–ø—É—Å –∏–∑ –õ–† ‚Ññ1.  \n",
        "- –†–µ–∞–ª–∏–∑—É–π—Ç–µ `text_preprocessing.py` —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:\n",
        "  - –û—á–∏—Å—Ç–∫–∏, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (`spaCy` –∏–ª–∏ `pymorphy2`),  \n",
        "  - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: whitespace, regex, **BPE (–∏–∑ –õ–† ‚Ññ1)**.\n",
        "\n",
        "> üí° **spaCy** –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 64-–±–∏—Ç–Ω—ã–µ Python 3.7+ –Ω–∞ Linux, macOS, Windows. –î–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –≤–Ω–µ—à–Ω–∏–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä.\n",
        "\n",
        "### **3.2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è**\n",
        "- –†–µ–∞–ª–∏–∑—É–π—Ç–µ `text_to_vector.py` —Å:\n",
        "  - `TfidfVectorizer` (scikit-learn)  \n",
        "  - `BM25Okapi` (rank-bm25)  \n",
        "  - –ó–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π –∏–∑ –õ–† ‚Ññ2: `Word2Vec`, `FastText`, `GloVe` (—á–µ—Ä–µ–∑ `Gensim`)  \n",
        "- –ù–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ –≤–µ–∫—Ç–æ—Ä—ã (`L2`) –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ cosine similarity.\n",
        "\n",
        "> üí° **Gensim** ‚Äî —Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (C-—è–¥—Ä–∞, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, streaming). –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Python 3.8+ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö.\n",
        "\n",
        "### **3.3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è**\n",
        "–†–µ–∞–ª–∏–∑—É–π—Ç–µ `clustering.py`, –≤–∫–ª—é—á–∞—é—â–∏–π **–≤—Å–µ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—à–µ –º–µ—Ç–æ–¥—ã**.  \n",
        "–î–ª—è –∫–∞–∂–¥–æ–≥–æ:\n",
        "- –ü–æ–¥–±–µ—Ä–∏—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (`k`, `eps`, `min_samples`, `n_components` –∏ –¥—Ä.)  \n",
        "- –ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–ª–∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—É—é –≤—ã–±–æ—Ä–∫—É –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ\n",
        "\n",
        "### **3.4. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞**\n",
        "- **–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏**: Silhouette, Calinski-Harabasz, Davies-Bouldin  \n",
        "- **–í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏** (–µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –∏–∑ –õ–† ‚Ññ3): ARI, NMI, V-measure  \n",
        "- –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "\n",
        "### **3.5. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**\n",
        "- –î–ª—è TF-IDF: —Ç–æ–ø-10 —Å–ª–æ–≤ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä  \n",
        "- –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –±–ª–∏–∂–∞–π—à–∏–µ —Å–ª–æ–≤–∞ –∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É (—á–µ—Ä–µ–∑ `model.most_similar()`)  \n",
        "- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: UMAP ‚Üí 2D scatter plot —Å —Ü–≤–µ—Ç–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "\n",
        "> üìå **LIME –∏ SHAP** –º–æ–≥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è **—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫ –≤ downstream-–∑–∞–¥–∞—á–µ** (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Ç–µ—Ä ‚Üí –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞). –í —á–∏—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –æ–Ω–∏ **–Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º—ã –Ω–∞–ø—Ä—è–º—É—é**, –Ω–æ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Å–≤–µ–Ω–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤.\n",
        "\n",
        "### **3.6. AutoML –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏?**\n",
        "> ‚ùó **–í–∞–∂–Ω–æ**: **AutoML-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ (Auto-sklearn, TPOT, H2O)** **–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –∑–∞–¥–∞—á–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è** (–≤–∫–ª—é—á–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é) –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.  \n",
        "> Auto-sklearn –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **—Ç–æ–ª—å–∫–æ –∑–∞–¥–∞—á–∏ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π** (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è).  \n",
        "> –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, **AutoML –≤ –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø**.\n",
        "\n",
        "### **3.7. –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `Streamlit` –∏–ª–∏ `Gradio` –¥–ª—è:\n",
        "  - –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏/–∞–ª–≥–æ—Ä–∏—Ç–º–∞,  \n",
        "  - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤,  \n",
        "  - –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è\n",
        "\n",
        "1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è BPE (–∏–∑ –õ–† ‚Ññ1) –∏ whitespace –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ  \n",
        "2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LDA –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è k-Means  \n",
        "3. –ì—Ä–∞—Ñ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ ‚Üí community detection  \n",
        "4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (silhouette analysis, gap statistic)\n",
        "\n",
        "---\n",
        "\n",
        "## üìÑ 5. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç—á—ë—Ç—É\n",
        "\n",
        "- –£–∫–∞–∂–∏—Ç–µ, —á—Ç–æ BPE –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Äî **—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ, –æ–±—É—á–µ–Ω–Ω—ã–µ –≤ –õ–† ‚Ññ1 –∏ ‚Ññ2**  \n",
        "- –ü—Ä–∏–≤–µ–¥–∏—Ç–µ **—Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤** –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ –≤—Ä–µ–º–µ–Ω–∏  \n",
        "- –í–∫–ª—é—á–∏—Ç–µ **–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏** –∏ **–ø—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤** –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤  \n",
        "- –°—Å—ã–ª–∫–∏ –Ω–∞ GitHub –∏ –≤–µ–±-–¥–µ–º–æ  \n",
        "- –†–µ—Ñ–ª–µ–∫—Å–∏—è:  \n",
        "  - –ö–∞–∫–æ–π –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ–∫–∞–∑–∞–ª –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞?  \n",
        "  - –ö–∞–∫ –≤–ª–∏—è–µ—Ç –≤—ã–±–æ—Ä –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Ñ–æ—Ä–º—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤?  \n",
        "  - –ü–æ—á–µ–º—É AutoML –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º –∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏?\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ 6. –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "\n",
        "| –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ | –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ |\n",
        "|-----------|-----------|-----------|\n",
        "| `Gensim` | Word2Vec, FastText, GloVe, LDA | –°—É–ø–µ—Ä–±—ã—Å—Ç—Ä–∞—è, streaming, –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–∞—è |\n",
        "| `scikit-learn` | TF-IDF, k-Means, DBSCAN, GMM, —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è | –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–µ-—Ñ–∞–∫—Ç–æ |\n",
        "| `hdbscan` | HDBSCAN | –î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã |\n",
        "| `spaCy` | –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, –±–∞–∑–æ–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è | –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π |\n",
        "| `subword-nmt` | BPE | –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è BPE |\n",
        "| `umap-learn` | –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ t-SNE |\n",
        "| `Streamlit` / `Gradio` | –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å | –ë—ã—Å—Ç—Ä–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ |\n",
        "\n"
      ],
      "metadata": {
        "id": "rIBFp_E9mxCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ 3. –ú–µ—Ç–æ–¥–∏–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
        "\n",
        "### **3.1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ—Ä–ø—É—Å –∏–∑ –õ–† ‚Ññ1.  \n",
        "- –†–µ–∞–ª–∏–∑—É–π—Ç–µ `text_preprocessing.py` —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:\n",
        "  - –û—á–∏—Å—Ç–∫–∏, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (`spaCy` –∏–ª–∏ `pymorphy2`),  \n",
        "  - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: whitespace, regex, **BPE (–∏–∑ –õ–† ‚Ññ1)**.\n",
        "\n",
        "> üí° **spaCy** –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 64-–±–∏—Ç–Ω—ã–µ Python 3.7+ –Ω–∞ Linux, macOS, Windows. –î–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –≤–Ω–µ—à–Ω–∏–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä.\n"
      ],
      "metadata": {
        "id": "M5bdlX30odFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet gensim\n"
      ],
      "metadata": {
        "id": "Pech7kFZfcO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pymorphy3\n",
        "import sentencepiece as spm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, FastText\n"
      ],
      "metadata": {
        "id": "CIzyhlModVKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ—Ä—É—Å\n",
        "!pip install corus\n",
        "\n",
        "# 2) –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ Lenta\n",
        "!wget -q https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
        "\n",
        "# 3) –ß—Ç–µ–Ω–∏–µ –∏ –ø–æ–¥—Å—á—ë—Ç –∑–∞–ø–∏—Å–µ–π\n",
        "import gzip\n",
        "from corus import load_lenta\n",
        "\n",
        "path = 'lenta-ru-news.csv.gz'\n",
        "records = list(load_lenta(path))\n",
        "print(\"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –∫–æ—Ä–ø—É—Å–µ Lenta:\", len(records))\n",
        "\n",
        "# 4) –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–µ—Ä–≤—ã–µ 5 —Ç–µ–∫—Å—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Ç–µ–∫—Å—Ç—ã)\n",
        "for rec in records[:5]:\n",
        "    print(\"URL:\", rec.url)\n",
        "    print(\"–ó–∞–≥–æ–ª–æ–≤–æ–∫:\", rec.title)\n",
        "    print(\"–¢–µ–∫—Å—Ç:\", rec.text[:200], \"‚Ä¶\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sOAVXOlFd0cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from tqdm import tqdm\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "def preprocess(text):\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± —á–µ—Ä–µ–∑ gensim)\n",
        "    tokens = gensim.utils.simple_preprocess(text, min_len=2, max_len=15)\n",
        "\n",
        "    # 3. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
        "    lemmas = [morph.parse(token)[0].normal_form for token in tokens]\n",
        "\n",
        "    # 4. –í–æ–∑–≤—Ä–∞—Ç —Å—Ç—Ä–æ–∫–∏\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º –∫–æ—Ä–ø—É—Å–∞ (–º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)\n",
        "texts = [rec.text for rec in records[:15000]]  # –ø–µ—Ä–≤—ã–µ 15 000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –õ–†\n",
        "preprocessed_texts = [preprocess(t) for t in tqdm(texts)]\n"
      ],
      "metadata": {
        "id": "DDdyOGVxg7YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –ø–µ—Ä–≤—ã–µ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "for i in range(3):\n",
        "    print(preprocessed_texts[i])\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "kHbr6dLog_Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# 1Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª\n",
        "with open(\"corpus_preprocessed.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in preprocessed_texts:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# 2Ô∏è‚É£ –û–±—É—á–∞–µ–º BPE –º–æ–¥–µ–ª—å\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input='corpus_preprocessed.txt',\n",
        "    model_prefix='bpe',\n",
        "    vocab_size=8000,    # —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è, –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å\n",
        "    model_type='bpe',   # —Ç–∏–ø –º–æ–¥–µ–ª–∏\n",
        "    character_coverage=1.0  # –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–∫—Ä—ã—Ç—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é BPE –º–æ–¥–µ–ª—å\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('bpe.model')\n",
        "\n",
        "# 4Ô∏è‚É£ –ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ BPE\n",
        "example = \"–í–∏—Ü–µ-–ø—Ä–µ–º—å–µ—Ä —Ä–∞—Å—Å–∫–∞–∑–∞–ª –æ —Å–º–µ—Ä—Ç–Ω–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∫–∞ –≤ —Ä–µ–≥–∏–æ–Ω–∞—Ö\"\n",
        "print(sp.encode(example, out_type=str))\n"
      ],
      "metadata": {
        "id": "HIkcZkW1i7S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "sentences = [doc.split() for doc in preprocessed_texts]\n",
        "\n",
        "# 1Ô∏è‚É£ Word2Vec\n",
        "w2v = Word2Vec(sentences, vector_size=100, window=5, sg=1, min_count=2, workers=4)\n",
        "print(\"–ü—Ä–∏–º–µ—Ä –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ –∫ '—Ä–∞–∫':\", w2v.wv.most_similar(\"—Ä–∞–∫\", topn=5))\n",
        "\n",
        "# 2Ô∏è‚É£ FastText\n",
        "ft = FastText(sentences, vector_size=100, window=5, sg=1, min_count=2, workers=4)\n",
        "print(\"–ü—Ä–∏–º–µ—Ä –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ –∫ '—Ä–∞–∫':\", ft.wv.most_similar(\"—Ä–∞–∫\", topn=5))\n",
        "\n",
        "# 3Ô∏è‚É£ –í–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ = —Å—Ä–µ–¥–Ω–µ–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º\n",
        "import numpy as np\n",
        "\n",
        "def doc_vector(model, doc):\n",
        "    vecs = [model.wv[word] for word in doc.split() if word in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
        "\n",
        "X_w2v = np.array([doc_vector(w2v, doc) for doc in preprocessed_texts])\n",
        "X_ft = np.array([doc_vector(ft, doc) for doc in preprocessed_texts])\n",
        "\n",
        "print(\"Word2Vec –º–∞—Ç—Ä–∏—Ü–∞:\", X_w2v.shape)\n",
        "print(\"FastText –º–∞—Ç—Ä–∏—Ü–∞:\", X_ft.shape)\n"
      ],
      "metadata": {
        "id": "SiP2bb4JjZuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile text_preprocessing.py\n",
        "import gensim\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "import sentencepiece as spm\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"bpe.model\")  # –µ—Å–ª–∏ –µ—Å—Ç—å –≥–æ—Ç–æ–≤–∞—è BPE\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = gensim.utils.simple_preprocess(text)\n",
        "    lemmas = [morph.parse(t)[0].normal_form for t in tokens]\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "def tokenize_bpe(text):\n",
        "    return sp.encode(text, out_type=str)\n",
        "\n"
      ],
      "metadata": {
        "id": "Zf2RSLialatS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from text_preprocessing import preprocess, tokenize_bpe\n",
        "\n",
        "doc = \"–í–∏—Ü–µ-–ø—Ä–µ–º—å–µ—Ä —Ä–∞—Å—Å–∫–∞–∑–∞–ª –æ —Å–º–µ—Ä—Ç–Ω–æ—Å—Ç–∏\"\n",
        "tokens = preprocess(doc)\n",
        "bpe_tokens = tokenize_bpe(doc)\n",
        "\n",
        "print(\"Preprocessed:\", tokens)\n",
        "print(\"BPE tokens:\", bpe_tokens)\n"
      ],
      "metadata": {
        "id": "-fatvheUlhxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from text_preprocessing import preprocess, tokenize_bpe\n",
        "from tqdm import tqdm\n",
        "\n",
        "# –ë–µ—Ä—ë–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "texts = [rec.text for rec in records[:15000]]\n",
        "\n",
        "# 1Ô∏è‚É£ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–æ—á–∏—Å—Ç–∫–∞ + –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)\n",
        "preprocessed_texts = [preprocess(doc) for doc in tqdm(texts)]\n",
        "\n",
        "# 2Ô∏è‚É£ BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞\n",
        "bpe_texts = [tokenize_bpe(doc) for doc in tqdm(preprocessed_texts)]\n",
        "\n",
        "# 3Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "for i in range(3):\n",
        "    print(\"–î–æ–∫—É–º–µ–Ω—Ç:\", preprocessed_texts[i])\n",
        "    print(\"BPE —Ç–æ–∫–µ–Ω—ã:\", bpe_texts[i][:20])\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ],
      "metadata": {
        "id": "gWdIgv1AmSmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **3.2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è**\n",
        "- –†–µ–∞–ª–∏–∑—É–π—Ç–µ `text_to_vector.py` —Å:\n",
        "  - `TfidfVectorizer` (scikit-learn)  \n",
        "  - `BM25Okapi` (rank-bm25)  \n",
        "  - –ó–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π –∏–∑ –õ–† ‚Ññ2: `Word2Vec`, `FastText`, `GloVe` (—á–µ—Ä–µ–∑ `Gensim`)  \n",
        "- –ù–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ –≤–µ–∫—Ç–æ—Ä—ã (`L2`) –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ cosine similarity.\n",
        "\n",
        "> üí° **Gensim** ‚Äî —Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (C-—è–¥—Ä–∞, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, streaming). –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Python 3.8+ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö."
      ],
      "metadata": {
        "id": "2qtXXvhwofdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile text_to_vector.py\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# --------------------------\n",
        "# TF-IDF\n",
        "# --------------------------\n",
        "def tfidf_vectorize(docs, max_features=10000):\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    X = vectorizer.fit_transform(docs)\n",
        "    X = normalize(X, norm='l2')  # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    return X, vectorizer\n",
        "\n",
        "# --------------------------\n",
        "# BM25\n",
        "# --------------------------\n",
        "def bm25_vectorize(tokenized_docs):\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "    return bm25\n",
        "\n",
        "# --------------------------\n",
        "# Word2Vec / FastText\n",
        "# --------------------------\n",
        "def train_word2vec(sentences, vector_size=100, window=5, sg=1, min_count=2):\n",
        "    model = Word2Vec(sentences, vector_size=vector_size, window=window,\n",
        "                     sg=sg, min_count=min_count, workers=4)\n",
        "    return model\n",
        "\n",
        "def train_fasttext(sentences, vector_size=100, window=5, sg=1, min_count=2):\n",
        "    model = FastText(sentences, vector_size=vector_size, window=window,\n",
        "                     sg=sg, min_count=min_count, workers=4)\n",
        "    return model\n",
        "\n",
        "# –°—Ä–µ–¥–Ω–µ–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "def doc_vector(model, doc_tokens):\n",
        "    vecs = [model.wv[token] for token in doc_tokens if token in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n"
      ],
      "metadata": {
        "id": "pj-NS8-Cowhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from text_to_vector import tfidf_vectorize, bm25_vectorize, train_word2vec, train_fasttext, doc_vector\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1Ô∏è‚É£ TF-IDF (—Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö)\n",
        "X_tfidf, tfidf_vectorizer = tfidf_vectorize(preprocessed_texts, max_features=10000)\n",
        "print(\"TF-IDF –º–∞—Ç—Ä–∏—Ü–∞:\", X_tfidf.shape)\n",
        "\n",
        "# 2Ô∏è‚É£ BM25 (—Ç—Ä–µ–±—É–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤)\n",
        "tokenized_docs = [doc.split() for doc in preprocessed_texts]\n",
        "bm25 = bm25_vectorize(tokenized_docs)\n",
        "print(\"BM25 –≥–æ—Ç–æ–≤\")\n",
        "\n",
        "# 3Ô∏è‚É£ Word2Vec\n",
        "w2v_model = train_word2vec(tokenized_docs, vector_size=100)\n",
        "X_w2v = np.array([doc_vector(w2v_model, doc) for doc in tqdm(tokenized_docs)])\n",
        "print(\"Word2Vec –º–∞—Ç—Ä–∏—Ü–∞:\", X_w2v.shape)\n",
        "\n",
        "# 4Ô∏è‚É£ FastText\n",
        "ft_model = train_fasttext(tokenized_docs, vector_size=100)\n",
        "X_ft = np.array([doc_vector(ft_model, doc) for doc in tqdm(tokenized_docs)])\n",
        "print(\"FastText –º–∞—Ç—Ä–∏—Ü–∞:\", X_ft.shape)\n"
      ],
      "metadata": {
        "id": "uvg5ZfDZpBgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile text_to_vector.py\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.preprocessing import normalize\n",
        "import gensim.downloader as api\n",
        "\n",
        "# --------------------------\n",
        "# TF-IDF\n",
        "# --------------------------\n",
        "def tfidf_vectorize(docs, max_features=10000):\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    X = vectorizer.fit_transform(docs)\n",
        "    X = normalize(X, norm='l2')  # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    return X, vectorizer\n",
        "\n",
        "# --------------------------\n",
        "# BM25\n",
        "# --------------------------\n",
        "def bm25_vectorize(tokenized_docs):\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "    return bm25\n",
        "\n",
        "# --------------------------\n",
        "# Word2Vec / FastText\n",
        "# --------------------------\n",
        "def train_word2vec(sentences, vector_size=100, window=5, sg=1, min_count=2):\n",
        "    model = Word2Vec(sentences, vector_size=vector_size, window=window,\n",
        "                     sg=sg, min_count=min_count, workers=4)\n",
        "    return model\n",
        "\n",
        "def train_fasttext(sentences, vector_size=100, window=5, sg=1, min_count=2):\n",
        "    model = FastText(sentences, vector_size=vector_size, window=window,\n",
        "                     sg=sg, min_count=min_count, workers=4)\n",
        "    return model\n",
        "\n",
        "def doc_vector(model, doc_tokens):\n",
        "    vecs = [model.wv[token] for token in doc_tokens if token in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
        "\n",
        "# --------------------------\n",
        "# GloVe\n",
        "# --------------------------\n",
        "def glove_vectorize(tokenized_docs, model_name=\"glove-wiki-gigaword-100\"):\n",
        "    glove_model = api.load(model_name)\n",
        "    vectors = []\n",
        "    for doc in tokenized_docs:\n",
        "        vecs = [glove_model[word] for word in doc if word in glove_model]\n",
        "        vectors.append(np.mean(vecs, axis=0) if vecs else np.zeros(glove_model.vector_size))\n",
        "    vectors = normalize(np.array(vectors), norm='l2')\n",
        "    return vectors, glove_model\n"
      ],
      "metadata": {
        "id": "butNu149rWyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from text_to_vector import tfidf_vectorize, bm25_vectorize, train_word2vec, train_fasttext, doc_vector, glove_vectorize\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1Ô∏è‚É£ TF-IDF\n",
        "X_tfidf, tfidf_vectorizer = tfidf_vectorize(preprocessed_texts)\n",
        "\n",
        "# 2Ô∏è‚É£ BM25\n",
        "tokenized_docs = [doc.split() for doc in preprocessed_texts]\n",
        "bm25 = bm25_vectorize(tokenized_docs)\n",
        "\n",
        "# 3Ô∏è‚É£ Word2Vec\n",
        "w2v_model = train_word2vec(tokenized_docs)\n",
        "X_w2v = np.array([doc_vector(w2v_model, doc) for doc in tqdm(tokenized_docs)])\n",
        "\n",
        "# 4Ô∏è‚É£ FastText\n",
        "ft_model = train_fasttext(tokenized_docs)\n",
        "X_ft = np.array([doc_vector(ft_model, doc) for doc in tqdm(tokenized_docs)])\n",
        "\n",
        "# 5Ô∏è‚É£ GloVe\n",
        "X_glove, glove_model = glove_vectorize(tokenized_docs)\n",
        "print(\"–í—Å–µ –º–∞—Ç—Ä–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã!\")\n"
      ],
      "metadata": {
        "id": "5Ol5qQjqreIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}