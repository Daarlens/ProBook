{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daarlens/ProBook/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22%D0%9B%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D1%83%D0%BC_%E2%84%96_4_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# üß™ **–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–π –ø—Ä–∞–∫—Ç–∏–∫—É–º ‚Ññ 4**  \n",
        "# **–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–∞—Ö**\n",
        "\n",
        "**–ö–∞—Ñ–µ–¥—Ä–∞:** –ö–∞—Ñ–µ–¥—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è  \n",
        "**–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞:** –û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞  \n",
        "**–£—Ä–æ–≤–µ–Ω—å:** –ú–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞, 2 –∫—É—Ä—Å  \n",
        "**–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å:** –ê—Ä–∞–±–æ–≤ –ú—É–ª–ª–æ—à–∞—Ä–∞—Ñ –ö—É—Ä–±–æ–Ω–æ–≤–∏—á  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ 1. –¶–µ–ª–∏ –∏ –∑–∞–¥–∞—á–∏ —Ä–∞–±–æ—Ç—ã\n",
        "\n",
        "### **–¶–µ–ª—å:**  \n",
        "–ü–æ–ª—É—á–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ **–≤—Å–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤**, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ. –ù–∞—É—á–∏—Ç—å—Å—è –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏.\n",
        "\n",
        "### **–ó–∞–¥–∞—á–∏:**  \n",
        "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—Ä–ø—É—Å –∏–∑ **–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã ‚Ññ1** (‚â•10 000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤).  \n",
        "2. –ü—Ä–∏–º–µ–Ω–∏—Ç—å **–º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏**, –≤–∫–ª—é—á–∞—è **—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é BPE-–º–æ–¥–µ–ª—å**, –æ–±—É—á–µ–Ω–Ω—É—é –≤ –õ–† ‚Ññ1.  \n",
        "3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–º–æ–¥–µ–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏**, –æ–±—É—á–µ–Ω–Ω—ã–µ –≤ **–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç–µ ‚Ññ2**:  \n",
        "   - Word2Vec (CBOW / Skip-Gram)  \n",
        "   - FastText (CBOW / Skip-Gram)  \n",
        "   - GloVe  \n",
        "4. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å **–≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏**:  \n",
        "   - **–¶–µ–Ω—Ç—Ä–æ–∏–¥–Ω—ã–µ**: k-Means, Mini-Batch k-Means, Spherical k-Means  \n",
        "   - **–ü–ª–æ—Ç–Ω–æ—Å—Ç–Ω—ã–µ**: DBSCAN, HDBSCAN  \n",
        "   - **–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ**: –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (single, complete, average, ward)  \n",
        "   - **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ**: Gaussian Mixture Models (GMM), Latent Dirichlet Allocation (LDA)  \n",
        "   - **–ì—Ä–∞—Ñ–æ–≤—ã–µ**: —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è  \n",
        "5. –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é **–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∏ –≤–Ω–µ—à–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫**.  \n",
        "6. –ü—Ä–æ–≤–µ—Å—Ç–∏ **–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é** –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.  \n",
        "7. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å **—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å** –∫ —à—É–º—É, –¥–∏—Å–±–∞–ª–∞–Ω—Å—É –∏ –æ–±—ä—ë–º—É –¥–∞–Ω–Ω—ã—Ö.  \n",
        "8. –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å **–≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö 2. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏\n",
        "\n",
        "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ ‚Äî –∑–∞–¥–∞—á–∞ **–æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è**, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –Ω–∞ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É **–±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏**. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è **—Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ, –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ –º–µ—Ç–æ–¥—ã**, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤ –∑—Ä–µ–ª—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ö (`scikit-learn`, `Gensim`, `hdbscan`).\n",
        "\n",
        "### **2.1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–∏–∑ –õ–† ‚Ññ1)**\n",
        "- Whitespace  \n",
        "- –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è  \n",
        "- **Byte Pair Encoding (BPE)** ‚Äî –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –õ–† ‚Ññ1 –º–æ–¥–µ–ª—å (—á–µ—Ä–µ–∑ `subword-nmt` –∏–ª–∏ –∞–Ω–∞–ª–æ–≥)\n",
        "\n",
        "### **2.2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (–∏–∑ –õ–† ‚Ññ2)**\n",
        "- **TF-IDF**, **BM25** ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞  \n",
        "- **Word2Vec**, **GloVe**, **FastText** ‚Äî —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –≤ –õ–† ‚Ññ2 –Ω–∞ —Ç–æ–º –∂–µ –∫–æ—Ä–ø—É—Å–µ  \n",
        "> –í–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ = —Å—Ä–µ–¥–Ω–µ–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º. –í—Å–µ –º–æ–¥–µ–ª–∏ ‚Äî **—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ**, –Ω–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –∏–∑–≤–Ω–µ.\n",
        "\n",
        "### **2.3. –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏**\n",
        "\n",
        "–í—Å–µ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ö:\n",
        "\n",
        "#### **–¶–µ–Ω—Ç—Ä–æ–∏–¥–Ω—ã–µ** (`scikit-learn`):\n",
        "- `KMeans` ‚Äî –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—É–º–º—É –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤  \n",
        "- `MiniBatchKMeans` ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö  \n",
        "- `SphericalKMeans` ‚Äî –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö (–∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö) –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤\n",
        "\n",
        "#### **–ü–ª–æ—Ç–Ω–æ—Å—Ç–Ω—ã–µ** (`scikit-learn`, `hdbscan`):\n",
        "- `DBSCAN` ‚Äî –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã, —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º  \n",
        "- `HDBSCAN` ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –≤–µ—Ä—Å–∏—è DBSCAN, **–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤**\n",
        "\n",
        "#### **–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ** (`scikit-learn`):\n",
        "- `AgglomerativeClustering` —Å:\n",
        "  - `linkage='ward'` (—Ç—Ä–µ–±—É–µ—Ç –µ–≤–∫–ª–∏–¥–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏)  \n",
        "  - `linkage='average'`, `'complete'`, `'single'` (—Ä–∞–±–æ—Ç–∞—é—Ç —Å `metric='cosine'`)\n",
        "\n",
        "#### **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ**:\n",
        "- `GaussianMixture` (`scikit-learn`) ‚Äî –º—è–≥–∫–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ç–æ—á–µ–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞–º  \n",
        "- `LdaModel` (`Gensim`) ‚Äî –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º–∞—è –∫–∞–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä –ø–æ —Ç–µ–º–∞–º\n",
        "\n",
        "#### **–ì—Ä–∞—Ñ–æ–≤—ã–µ** (`scikit-learn`):\n",
        "- `SpectralClustering` ‚Äî –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏\n",
        "\n",
        "> ‚ö†Ô∏è –í—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Å `numpy.ndarray`, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö **—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º–∏** –¥–ª—è –ª—é–±–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ 3. –ú–µ—Ç–æ–¥–∏–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
        "\n",
        "### **3.1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ—Ä–ø—É—Å –∏–∑ –õ–† ‚Ññ1.  \n",
        "- –†–µ–∞–ª–∏–∑—É–π—Ç–µ `text_preprocessing.py` —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:\n",
        "  - –û—á–∏—Å—Ç–∫–∏, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (`spaCy` –∏–ª–∏ `pymorphy2`),  \n",
        "  - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: whitespace, regex, **BPE (–∏–∑ –õ–† ‚Ññ1)**.\n",
        "\n",
        "> üí° **spaCy** –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 64-–±–∏—Ç–Ω—ã–µ Python 3.7+ –Ω–∞ Linux, macOS, Windows. –î–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –≤–Ω–µ—à–Ω–∏–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä.\n",
        "\n",
        "### **3.2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è**\n",
        "- –†–µ–∞–ª–∏–∑—É–π—Ç–µ `text_to_vector.py` —Å:\n",
        "  - `TfidfVectorizer` (scikit-learn)  \n",
        "  - `BM25Okapi` (rank-bm25)  \n",
        "  - –ó–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π –∏–∑ –õ–† ‚Ññ2: `Word2Vec`, `FastText`, `GloVe` (—á–µ—Ä–µ–∑ `Gensim`)  \n",
        "- –ù–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ –≤–µ–∫—Ç–æ—Ä—ã (`L2`) –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ cosine similarity.\n",
        "\n",
        "> üí° **Gensim** ‚Äî —Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (C-—è–¥—Ä–∞, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, streaming). –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Python 3.8+ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö.\n",
        "\n",
        "### **3.3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è**\n",
        "–†–µ–∞–ª–∏–∑—É–π—Ç–µ `clustering.py`, –≤–∫–ª—é—á–∞—é—â–∏–π **–≤—Å–µ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—à–µ –º–µ—Ç–æ–¥—ã**.  \n",
        "–î–ª—è –∫–∞–∂–¥–æ–≥–æ:\n",
        "- –ü–æ–¥–±–µ—Ä–∏—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (`k`, `eps`, `min_samples`, `n_components` –∏ –¥—Ä.)  \n",
        "- –ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–ª–∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—É—é –≤—ã–±–æ—Ä–∫—É –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ\n",
        "\n",
        "### **3.4. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞**\n",
        "- **–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏**: Silhouette, Calinski-Harabasz, Davies-Bouldin  \n",
        "- **–í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏** (–µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –∏–∑ –õ–† ‚Ññ3): ARI, NMI, V-measure  \n",
        "- –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "\n",
        "### **3.5. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**\n",
        "- –î–ª—è TF-IDF: —Ç–æ–ø-10 —Å–ª–æ–≤ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä  \n",
        "- –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –±–ª–∏–∂–∞–π—à–∏–µ —Å–ª–æ–≤–∞ –∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É (—á–µ—Ä–µ–∑ `model.most_similar()`)  \n",
        "- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: UMAP ‚Üí 2D scatter plot —Å —Ü–≤–µ—Ç–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "\n",
        "> üìå **LIME –∏ SHAP** –º–æ–≥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è **—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫ –≤ downstream-–∑–∞–¥–∞—á–µ** (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Ç–µ—Ä ‚Üí –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞). –í —á–∏—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –æ–Ω–∏ **–Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º—ã –Ω–∞–ø—Ä—è–º—É—é**, –Ω–æ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Å–≤–µ–Ω–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤.\n",
        "\n",
        "### **3.6. AutoML –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏?**\n",
        "> ‚ùó **–í–∞–∂–Ω–æ**: **AutoML-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ (Auto-sklearn, TPOT, H2O)** **–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –∑–∞–¥–∞—á–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è** (–≤–∫–ª—é—á–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é) –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.  \n",
        "> Auto-sklearn –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **—Ç–æ–ª—å–∫–æ –∑–∞–¥–∞—á–∏ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π** (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è).  \n",
        "> –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, **AutoML –≤ –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø**.\n",
        "\n",
        "### **3.7. –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `Streamlit` –∏–ª–∏ `Gradio` –¥–ª—è:\n",
        "  - –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏/–∞–ª–≥–æ—Ä–∏—Ç–º–∞,  \n",
        "  - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤,  \n",
        "  - –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è\n",
        "\n",
        "1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è BPE (–∏–∑ –õ–† ‚Ññ1) –∏ whitespace –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ  \n",
        "2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LDA –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è k-Means  \n",
        "3. –ì—Ä–∞—Ñ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ ‚Üí community detection  \n",
        "4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (silhouette analysis, gap statistic)\n",
        "\n",
        "---\n",
        "\n",
        "## üìÑ 5. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç—á—ë—Ç—É\n",
        "\n",
        "- –£–∫–∞–∂–∏—Ç–µ, —á—Ç–æ BPE –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Äî **—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ, –æ–±—É—á–µ–Ω–Ω—ã–µ –≤ –õ–† ‚Ññ1 –∏ ‚Ññ2**  \n",
        "- –ü—Ä–∏–≤–µ–¥–∏—Ç–µ **—Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤** –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ –≤—Ä–µ–º–µ–Ω–∏  \n",
        "- –í–∫–ª—é—á–∏—Ç–µ **–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏** –∏ **–ø—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤** –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤  \n",
        "- –°—Å—ã–ª–∫–∏ –Ω–∞ GitHub –∏ –≤–µ–±-–¥–µ–º–æ  \n",
        "- –†–µ—Ñ–ª–µ–∫—Å–∏—è:  \n",
        "  - –ö–∞–∫–æ–π –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ–∫–∞–∑–∞–ª –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞?  \n",
        "  - –ö–∞–∫ –≤–ª–∏—è–µ—Ç –≤—ã–±–æ—Ä –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Ñ–æ—Ä–º—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤?  \n",
        "  - –ü–æ—á–µ–º—É AutoML –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º –∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏?\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ 6. –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "\n",
        "| –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ | –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ |\n",
        "|-----------|-----------|-----------|\n",
        "| `Gensim` | Word2Vec, FastText, GloVe, LDA | –°—É–ø–µ—Ä–±—ã—Å—Ç—Ä–∞—è, streaming, –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–∞—è |\n",
        "| `scikit-learn` | TF-IDF, k-Means, DBSCAN, GMM, —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è | –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–µ-—Ñ–∞–∫—Ç–æ |\n",
        "| `hdbscan` | HDBSCAN | –î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã |\n",
        "| `spaCy` | –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, –±–∞–∑–æ–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è | –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π |\n",
        "| `subword-nmt` | BPE | –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è BPE |\n",
        "| `umap-learn` | –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ t-SNE |\n",
        "| `Streamlit` / `Gradio` | –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å | –ë—ã—Å—Ç—Ä–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ |\n",
        "\n"
      ],
      "metadata": {
        "id": "rIBFp_E9mxCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ 3. –ú–µ—Ç–æ–¥–∏–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
        "\n",
        "### **3.1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ—Ä–ø—É—Å –∏–∑ –õ–† ‚Ññ1.  \n",
        "- –†–µ–∞–ª–∏–∑—É–π—Ç–µ `text_preprocessing.py` —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:\n",
        "  - –û—á–∏—Å—Ç–∫–∏, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (`spaCy` –∏–ª–∏ `pymorphy2`),  \n",
        "  - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: whitespace, regex, **BPE (–∏–∑ –õ–† ‚Ññ1)**.\n",
        "\n",
        "> üí° **spaCy** –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 64-–±–∏—Ç–Ω—ã–µ Python 3.7+ –Ω–∞ Linux, macOS, Windows. –î–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –≤–Ω–µ—à–Ω–∏–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä.\n"
      ],
      "metadata": {
        "id": "M5bdlX30odFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet gensim\n"
      ],
      "metadata": {
        "id": "Pech7kFZfcO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3\n"
      ],
      "metadata": {
        "id": "1TV7LcdBZQg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pymorphy3\n",
        "import sentencepiece as spm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, FastText\n"
      ],
      "metadata": {
        "id": "CIzyhlModVKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ—Ä—É—Å\n",
        "!pip install corus\n",
        "\n",
        "# 2) –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ Lenta\n",
        "!wget -q https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
        "\n",
        "# 3) –ß—Ç–µ–Ω–∏–µ –∏ –ø–æ–¥—Å—á—ë—Ç –∑–∞–ø–∏—Å–µ–π\n",
        "import gzip\n",
        "from corus import load_lenta\n",
        "\n",
        "path = 'lenta-ru-news.csv.gz'\n",
        "records = list(load_lenta(path))\n",
        "print(\"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –∫–æ—Ä–ø—É—Å–µ Lenta:\", len(records))\n",
        "\n",
        "# 4) –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–µ—Ä–≤—ã–µ 5 —Ç–µ–∫—Å—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Ç–µ–∫—Å—Ç—ã)\n",
        "for rec in records[:5]:\n",
        "    print(\"URL:\", rec.url)\n",
        "    print(\"–ó–∞–≥–æ–ª–æ–≤–æ–∫:\", rec.title)\n",
        "    print(\"–¢–µ–∫—Å—Ç:\", rec.text[:200], \"‚Ä¶\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sOAVXOlFd0cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from tqdm import tqdm\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "def preprocess(text):\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± —á–µ—Ä–µ–∑ gensim)\n",
        "    tokens = gensim.utils.simple_preprocess(text, min_len=2, max_len=15)\n",
        "\n",
        "    # 3. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
        "    lemmas = [morph.parse(token)[0].normal_form for token in tokens]\n",
        "\n",
        "    # 4. –í–æ–∑–≤—Ä–∞—Ç —Å—Ç—Ä–æ–∫–∏\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º –∫–æ—Ä–ø—É—Å–∞ (–º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)\n",
        "texts = [rec.text for rec in records[:15000]]  # –ø–µ—Ä–≤—ã–µ 15 000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –õ–†\n",
        "preprocessed_texts = [preprocess(t) for t in tqdm(texts)]\n"
      ],
      "metadata": {
        "id": "DDdyOGVxg7YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –ø–µ—Ä–≤—ã–µ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "for i in range(3):\n",
        "    print(preprocessed_texts[i])\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "kHbr6dLog_Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# 1Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª\n",
        "with open(\"corpus_preprocessed.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in preprocessed_texts:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# 2Ô∏è‚É£ –û–±—É—á–∞–µ–º BPE –º–æ–¥–µ–ª—å\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input='corpus_preprocessed.txt',\n",
        "    model_prefix='bpe',\n",
        "    vocab_size=8000,    # —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è, –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å\n",
        "    model_type='bpe',   # —Ç–∏–ø –º–æ–¥–µ–ª–∏\n",
        "    character_coverage=1.0  # –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–∫—Ä—ã—Ç—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é BPE –º–æ–¥–µ–ª—å\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('bpe.model')\n",
        "\n",
        "# 4Ô∏è‚É£ –ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ BPE\n",
        "example = \"–í–∏—Ü–µ-–ø—Ä–µ–º—å–µ—Ä —Ä–∞—Å—Å–∫–∞–∑–∞–ª –æ —Å–º–µ—Ä—Ç–Ω–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∫–∞ –≤ —Ä–µ–≥–∏–æ–Ω–∞—Ö\"\n",
        "print(sp.encode(example, out_type=str))\n"
      ],
      "metadata": {
        "id": "HIkcZkW1i7S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "sentences = [doc.split() for doc in preprocessed_texts]\n",
        "\n",
        "# 1Ô∏è‚É£ Word2Vec\n",
        "w2v = Word2Vec(sentences, vector_size=100, window=5, sg=1, min_count=2, workers=4)\n",
        "print(\"–ü—Ä–∏–º–µ—Ä –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ –∫ '—Ä–∞–∫':\", w2v.wv.most_similar(\"—Ä–∞–∫\", topn=5))\n",
        "\n",
        "# 2Ô∏è‚É£ FastText\n",
        "ft = FastText(sentences, vector_size=100, window=5, sg=1, min_count=2, workers=4)\n",
        "print(\"–ü—Ä–∏–º–µ—Ä –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ –∫ '—Ä–∞–∫':\", ft.wv.most_similar(\"—Ä–∞–∫\", topn=5))\n",
        "\n",
        "# 3Ô∏è‚É£ –í–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ = —Å—Ä–µ–¥–Ω–µ–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º\n",
        "import numpy as np\n",
        "\n",
        "def doc_vector(model, doc):\n",
        "    vecs = [model.wv[word] for word in doc.split() if word in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
        "\n",
        "X_w2v = np.array([doc_vector(w2v, doc) for doc in preprocessed_texts])\n",
        "X_ft = np.array([doc_vector(ft, doc) for doc in preprocessed_texts])\n",
        "\n",
        "print(\"Word2Vec –º–∞—Ç—Ä–∏—Ü–∞:\", X_w2v.shape)\n",
        "print(\"FastText –º–∞—Ç—Ä–∏—Ü–∞:\", X_ft.shape)\n"
      ],
      "metadata": {
        "id": "SiP2bb4JjZuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile text_preprocessing.py\n",
        "import gensim\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "import sentencepiece as spm\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"bpe.model\")  # –µ—Å–ª–∏ –µ—Å—Ç—å –≥–æ—Ç–æ–≤–∞—è BPE\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = gensim.utils.simple_preprocess(text)\n",
        "    lemmas = [morph.parse(t)[0].normal_form for t in tokens]\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "def tokenize_bpe(text):\n",
        "    return sp.encode(text, out_type=str)\n",
        "\n"
      ],
      "metadata": {
        "id": "Zf2RSLialatS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from text_preprocessing import preprocess, tokenize_bpe\n",
        "\n",
        "doc = \"–í–∏—Ü–µ-–ø—Ä–µ–º—å–µ—Ä —Ä–∞—Å—Å–∫–∞–∑–∞–ª –æ —Å–º–µ—Ä—Ç–Ω–æ—Å—Ç–∏\"\n",
        "tokens = preprocess(doc)\n",
        "bpe_tokens = tokenize_bpe(doc)\n",
        "\n",
        "print(\"Preprocessed:\", tokens)\n",
        "print(\"BPE tokens:\", bpe_tokens)\n"
      ],
      "metadata": {
        "id": "-fatvheUlhxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from text_preprocessing import preprocess, tokenize_bpe\n",
        "from tqdm import tqdm\n",
        "\n",
        "# –ë–µ—Ä—ë–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "texts = [rec.text for rec in records[:15000]]\n",
        "\n",
        "# 1Ô∏è‚É£ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–æ—á–∏—Å—Ç–∫–∞ + –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)\n",
        "preprocessed_texts = [preprocess(doc) for doc in tqdm(texts)]\n",
        "\n",
        "# 2Ô∏è‚É£ BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞\n",
        "bpe_texts = [tokenize_bpe(doc) for doc in tqdm(preprocessed_texts)]\n",
        "\n",
        "# 3Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "for i in range(3):\n",
        "    print(\"–î–æ–∫—É–º–µ–Ω—Ç:\", preprocessed_texts[i])\n",
        "    print(\"BPE —Ç–æ–∫–µ–Ω—ã:\", bpe_texts[i][:20])\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ],
      "metadata": {
        "id": "gWdIgv1AmSmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **3.2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è**\n",
        "- –†–µ–∞–ª–∏–∑—É–π—Ç–µ `text_to_vector.py` —Å:\n",
        "  - `TfidfVectorizer` (scikit-learn)  \n",
        "  - `BM25Okapi` (rank-bm25)  \n",
        "  - –ó–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π –∏–∑ –õ–† ‚Ññ2: `Word2Vec`, `FastText`, `GloVe` (—á–µ—Ä–µ–∑ `Gensim`)  \n",
        "- –ù–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ –≤–µ–∫—Ç–æ—Ä—ã (`L2`) –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ cosine similarity.\n",
        "\n",
        "> üí° **Gensim** ‚Äî —Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (C-—è–¥—Ä–∞, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, streaming). –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Python 3.8+ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö."
      ],
      "metadata": {
        "id": "2qtXXvhwofdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile text_to_vector.py\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.preprocessing import normalize\n",
        "import gensim.downloader as api\n",
        "\n",
        "# --------------------------\n",
        "# TF-IDF\n",
        "# --------------------------\n",
        "def tfidf_vectorize(docs, max_features=10000):\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    X = vectorizer.fit_transform(docs)\n",
        "    X = normalize(X, norm='l2')  # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    return X, vectorizer\n",
        "\n",
        "# --------------------------\n",
        "# BM25\n",
        "# --------------------------\n",
        "def bm25_vectorize(tokenized_docs):\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "    return bm25\n",
        "\n",
        "# --------------------------\n",
        "# Word2Vec / FastText\n",
        "# --------------------------\n",
        "def train_word2vec(sentences, vector_size=100, window=5, sg=1, min_count=2):\n",
        "    model = Word2Vec(sentences, vector_size=vector_size, window=window,\n",
        "                     sg=sg, min_count=min_count, workers=4)\n",
        "    return model\n",
        "\n",
        "def train_fasttext(sentences, vector_size=100, window=5, sg=1, min_count=2):\n",
        "    model = FastText(sentences, vector_size=vector_size, window=window,\n",
        "                     sg=sg, min_count=min_count, workers=4)\n",
        "    return model\n",
        "\n",
        "def doc_vector(model, doc_tokens):\n",
        "    vecs = [model.wv[token] for token in doc_tokens if token in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
        "\n",
        "# --------------------------\n",
        "# GloVe\n",
        "# --------------------------\n",
        "def glove_vectorize(tokenized_docs, model_name=\"glove-wiki-gigaword-100\"):\n",
        "    glove_model = api.load(model_name)\n",
        "    vectors = []\n",
        "    for doc in tokenized_docs:\n",
        "        vecs = [glove_model[word] for word in doc if word in glove_model]\n",
        "        vectors.append(np.mean(vecs, axis=0) if vecs else np.zeros(glove_model.vector_size))\n",
        "    vectors = normalize(np.array(vectors), norm='l2')\n",
        "    return vectors, glove_model\n"
      ],
      "metadata": {
        "id": "butNu149rWyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n"
      ],
      "metadata": {
        "id": "txNamFK8YQuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from text_to_vector import tfidf_vectorize, bm25_vectorize, train_word2vec, train_fasttext, doc_vector, glove_vectorize\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "\n",
        "# 1Ô∏è‚É£ TF-IDF\n",
        "X_tfidf, tfidf_vectorizer = tfidf_vectorize(preprocessed_texts)\n",
        "\n",
        "# 2Ô∏è‚É£ BM25\n",
        "tokenized_docs = [doc.split() for doc in preprocessed_texts]\n",
        "bm25 = bm25_vectorize(tokenized_docs)\n",
        "\n",
        "# 3Ô∏è‚É£ Word2Vec\n",
        "w2v_model = train_word2vec(tokenized_docs)\n",
        "X_w2v = np.array([doc_vector(w2v_model, doc) for doc in tqdm(tokenized_docs)])\n",
        "\n",
        "# 4Ô∏è‚É£ FastText\n",
        "ft_model = train_fasttext(tokenized_docs)\n",
        "X_ft = np.array([doc_vector(ft_model, doc) for doc in tqdm(tokenized_docs)])\n",
        "\n",
        "# 5Ô∏è‚É£ GloVe\n",
        "X_glove, glove_model = glove_vectorize(tokenized_docs)\n",
        "print(\"–í—Å–µ –º–∞—Ç—Ä–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã!\")\n"
      ],
      "metadata": {
        "id": "5Ol5qQjqreIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF:\", X_tfidf.shape)\n",
        "print(\"Word2Vec:\", X_w2v.shape)\n",
        "print(\"FastText:\", X_ft.shape)\n",
        "print(\"GloVe:\", X_glove.shape)\n"
      ],
      "metadata": {
        "id": "Zs4rcBVkbfSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è**\n",
        "–†–µ–∞–ª–∏–∑—É–π—Ç–µ `clustering.py`, –≤–∫–ª—é—á–∞—é—â–∏–π **–≤—Å–µ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—à–µ –º–µ—Ç–æ–¥—ã**.  \n",
        "–î–ª—è –∫–∞–∂–¥–æ–≥–æ:\n",
        "- –ü–æ–¥–±–µ—Ä–∏—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (`k`, `eps`, `min_samples`, `n_components` –∏ –¥—Ä.)  \n",
        "- –ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–ª–∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—É—é –≤—ã–±–æ—Ä–∫—É –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ"
      ],
      "metadata": {
        "id": "y9Y6ie8b6yF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "import hdbscan\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ –í—ã–±–æ—Ä –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è —Ç–µ—Å—Ç–∞\n",
        "# -----------------------------\n",
        "sample_size = 2000\n",
        "X_sample = X_w2v[:sample_size]  # –º–æ–∂–Ω–æ —Å–º–µ–Ω–∏—Ç—å –Ω–∞ X_ft –∏–ª–∏ X_tfidf\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –≤—ã–≤–æ–¥–∞ –º–µ—Ç—Ä–∏–∫\n",
        "# -----------------------------\n",
        "def evaluate_clusters(X, labels, name):\n",
        "    if len(set(labels)) <= 1:  # –ï—Å–ª–∏ –≤—Å–µ —Ç–æ—á–∫–∏ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
        "        print(f\"{name}: —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä\")\n",
        "        return\n",
        "    sil = silhouette_score(X, labels)\n",
        "    ch = calinski_harabasz_score(X, labels)\n",
        "    db = davies_bouldin_score(X, labels)\n",
        "    print(f\"{name} ‚Üí k={len(set(labels))}, Silhouette: {sil:.3f}, Calinski-Harabasz: {ch:.1f}, Davies-Bouldin: {db:.3f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ –¶–µ–Ω—Ç—Ä–æ–∏–¥–Ω—ã–µ –º–µ—Ç–æ–¥—ã\n",
        "# -----------------------------\n",
        "k = 10  # –ø—Ä–∏–º–µ—Ä\n",
        "km = KMeans(n_clusters=k, random_state=42).fit(X_sample)\n",
        "evaluate_clusters(X_sample, km.labels_, \"KMeans\")\n",
        "\n",
        "mbkm = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=256).fit(X_sample)\n",
        "evaluate_clusters(X_sample, mbkm.labels_, \"MiniBatchKMeans\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ\n",
        "# -----------------------------\n",
        "agg = AgglomerativeClustering(n_clusters=k, linkage='ward').fit(X_sample)\n",
        "evaluate_clusters(X_sample, agg.labels_, \"Agglomerative Ward\")\n",
        "\n",
        "agg_avg = AgglomerativeClustering(n_clusters=k, linkage='average', metric='cosine').fit(X_sample)\n",
        "evaluate_clusters(X_sample, agg_avg.labels_, \"Agglomerative Average\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ –ü–ª–æ—Ç–Ω–æ—Å—Ç–Ω—ã–µ\n",
        "# -----------------------------\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X_sample)\n",
        "evaluate_clusters(X_sample, dbscan.labels_, \"DBSCAN\")\n",
        "\n",
        "hdb = hdbscan.HDBSCAN(min_cluster_size=10).fit(X_sample)\n",
        "evaluate_clusters(X_sample, hdb.labels_, \"HDBSCAN\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ\n",
        "# -----------------------------\n",
        "gmm = GaussianMixture(n_components=k, random_state=42).fit(X_sample)\n",
        "gmm_labels = gmm.predict(X_sample)\n",
        "evaluate_clusters(X_sample, gmm_labels, \"GMM\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ –ì—Ä–∞—Ñ–æ–≤—ã–µ\n",
        "# -----------------------------\n",
        "sc = SpectralClustering(n_clusters=k, affinity='nearest_neighbors', random_state=42).fit(X_sample)\n",
        "evaluate_clusters(X_sample, sc.labels_, \"SpectralClustering\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è UMAP\n",
        "# -----------------------------\n",
        "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
        "X_2d = reducer.fit_transform(X_sample)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(X_2d[:,0], X_2d[:,1], c=km.labels_, cmap='tab10', s=5)\n",
        "plt.title(\"KMeans clusters visualized with UMAP\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CHOwMTsn6y_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile clustering2.py\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from scipy.sparse import issparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_clusters(X, labels, name, sample_size=500):\n",
        "    n_clusters = len(set(labels))\n",
        "    if n_clusters < 2:\n",
        "        print(f\"{name} ‚Üí —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –º–µ—Ç—Ä–∏–∫\")\n",
        "        return\n",
        "    # –ï—Å–ª–∏ X —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è, –±–µ—Ä–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è –º–µ—Ç—Ä–∏–∫\n",
        "    if issparse(X):\n",
        "        X_sample = X[:sample_size].toarray()\n",
        "        labels_sample = labels[:sample_size]\n",
        "    else:\n",
        "        X_sample = X\n",
        "        labels_sample = labels\n",
        "    sil = silhouette_score(X_sample, labels_sample)\n",
        "    ch = calinski_harabasz_score(X_sample, labels_sample)\n",
        "    db = davies_bouldin_score(X_sample, labels_sample)\n",
        "    print(f\"{name} ‚Üí k={n_clusters}, Silhouette: {sil:.3f}, Calinski-Harabasz: {ch:.1f}, Davies-Bouldin: {db:.3f}\")\n",
        "\n",
        "def run_all_clusters(X, k=10):\n",
        "    # ------------------ Sparse-friendly ------------------\n",
        "    if issparse(X):\n",
        "        print(\"Sparse matrix detected ‚Üí –∑–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ KMeans / MiniBatchKMeans\")\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n",
        "        evaluate_clusters(X, kmeans.labels_, \"KMeans\")\n",
        "\n",
        "        mbk = MiniBatchKMeans(n_clusters=k, random_state=42).fit(X)\n",
        "        evaluate_clusters(X, mbk.labels_, \"MiniBatchKMeans\")\n",
        "        return\n",
        "\n",
        "    # ------------------ Dense ------------------\n",
        "    print(\"Dense matrix detected ‚Üí –∑–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã\")\n",
        "\n",
        "    # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (–≤–∞–∂–Ω–æ –¥–ª—è GloVe)\n",
        "    nonzero_mask = ~(X == 0).all(axis=1)\n",
        "    X_dense = X[nonzero_mask]\n",
        "\n",
        "    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ—Ç –≤–µ–∫—Ç–æ—Ä–æ–≤, –ø—Ä–æ—Å—Ç–æ –≤—ã—Ö–æ–¥–∏–º\n",
        "    if X_dense.shape[0] == 0:\n",
        "        print(\"–í—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –Ω—É–ª–µ–≤—ã–µ ‚Üí –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é\")\n",
        "        return\n",
        "\n",
        "    # KMeans\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_dense)\n",
        "    evaluate_clusters(X_dense, kmeans.labels_, \"KMeans\")\n",
        "\n",
        "    # MiniBatchKMeans\n",
        "    mbk = MiniBatchKMeans(n_clusters=k, random_state=42).fit(X_dense)\n",
        "    evaluate_clusters(X_dense, mbk.labels_, \"MiniBatchKMeans\")\n",
        "\n",
        "    # Agglomerative\n",
        "    agg_ward = AgglomerativeClustering(n_clusters=k, linkage='ward').fit(X_dense)\n",
        "    evaluate_clusters(X_dense, agg_ward.labels_, \"Agglomerative Ward\")\n",
        "\n",
        "    agg_avg = AgglomerativeClustering(n_clusters=k, linkage='average', metric='cosine').fit(X_dense)\n",
        "    evaluate_clusters(X_dense, agg_avg.labels_, \"Agglomerative Average\")\n",
        "\n",
        "    # DBSCAN\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5, metric='cosine').fit(X_dense)\n",
        "    evaluate_clusters(X_dense, dbscan.labels_, \"DBSCAN\")\n",
        "\n",
        "    # HDBSCAN\n",
        "    hdb = HDBSCAN(min_cluster_size=10, metric='euclidean').fit(X_dense)\n",
        "    evaluate_clusters(X_dense, hdb.labels_, \"HDBSCAN\")\n",
        "\n",
        "    # Gaussian Mixture\n",
        "    gmm = GaussianMixture(n_components=k, random_state=42).fit(X_dense)\n",
        "    labels_gmm = gmm.predict(X_dense)\n",
        "    evaluate_clusters(X_dense, labels_gmm, \"GMM\")\n",
        "\n",
        "    # Spectral Clustering\n",
        "    spectral = SpectralClustering(n_clusters=k, affinity='nearest_neighbors', random_state=42).fit(X_dense)\n",
        "    evaluate_clusters(X_dense, spectral.labels_, \"SpectralClustering\")\n"
      ],
      "metadata": {
        "id": "CoQyhXsS72p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from clustering2 import run_all_clusters\n",
        "\n",
        "# –î–ª—è TF-IDF (sparse)\n",
        "run_all_clusters(X_tfidf, k=10)\n",
        "\n",
        "# –î–ª—è Word2Vec / FastText / GloVe (dense)\n",
        "run_all_clusters(X_w2v, k=10)\n",
        "run_all_clusters(X_ft, k=10)\n",
        "run_all_clusters(X_glove, k=10)\n"
      ],
      "metadata": {
        "id": "pemJamUm9Db5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from clustering2 import run_all_clusters\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä: KMeans –Ω–∞ –ø–ª–æ—Ç–Ω–æ–π –º–∞—Ç—Ä–∏—Ü–µ\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k = 10\n",
        "kmeans = KMeans(n_clusters=k, random_state=42).fit(X_w2v)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ DataFrame\n",
        "df_clusters = pd.DataFrame({'doc_id': range(len(labels)), 'cluster': labels})\n",
        "print(df_clusters['cluster'].value_counts())\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_vis = pca.fit_transform(X_w2v)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_vis[:,0], X_vis[:,1], c=labels, cmap='tab10', s=5)\n",
        "plt.title(\"KMeans –Ω–∞ Word2Vec\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kf2tQsOTWHqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **3.4. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞**\n",
        "- **–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏**: Silhouette, Calinski-Harabasz, Davies-Bouldin  \n",
        "- **–í–Ω–µ—à–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏** (–µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –∏–∑ –õ–† ‚Ññ3): ARI, NMI, V-measure  \n",
        "- –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n"
      ],
      "metadata": {
        "id": "eTohV5unWWV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫"
      ],
      "metadata": {
        "id": "1FziiJRuiaNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from scipy.sparse import issparse\n",
        "\n",
        "def plot_internal_metrics(X, k_range, title=\"\"):\n",
        "    sil_scores, ch_scores, db_scores = [], [], []\n",
        "\n",
        "    # –ü—Ä–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü–µ –±–µ—Ä—ë–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É (–º–µ—Ç—Ä–∏–∫–∏ —Ç—Ä–µ–±—É—é—Ç dense)\n",
        "    if issparse(X):\n",
        "        X_eval = X[:1500].toarray()\n",
        "    else:\n",
        "        X_eval = X\n",
        "\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42).fit(X_eval)\n",
        "        labels = kmeans.labels_\n",
        "        sil_scores.append(silhouette_score(X_eval, labels))\n",
        "        ch_scores.append(calinski_harabasz_score(X_eval, labels))\n",
        "        db_scores.append(davies_bouldin_score(X_eval, labels))\n",
        "\n",
        "    plt.figure(figsize=(15,4))\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.plot(k_range, sil_scores, marker='o')\n",
        "    plt.title(f\"Silhouette ({title})\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(k_range, ch_scores, marker='o')\n",
        "    plt.title(f\"Calinski‚ÄìHarabasz ({title})\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot(k_range, db_scores, marker='o')\n",
        "    plt.title(f\"Davies‚ÄìBouldin ({title})\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "h9N76L2dXf3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå TF-IDF (—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è)"
      ],
      "metadata": {
        "id": "kiRkYchUihZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_internal_metrics(X_tfidf, k_range=range(2, 20, 2), title=\"TF-IDF\")\n"
      ],
      "metadata": {
        "id": "vz7ierGYiiKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Word2Vec"
      ],
      "metadata": {
        "id": "u5r1d7reikHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_internal_metrics(X_w2v, k_range=range(2, 20, 2), title=\"Word2Vec\")\n"
      ],
      "metadata": {
        "id": "tMdroihMioOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå FastText"
      ],
      "metadata": {
        "id": "duRFmn9UiyLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_internal_metrics(X_ft, k_range=range(2, 20, 2), title=\"FastText\")\n"
      ],
      "metadata": {
        "id": "m8YK4E6wi0J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå GloVe"
      ],
      "metadata": {
        "id": "YhyNUcYzi3ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_internal_metrics(X_glove, k_range=range(2, 20, 2), title=\"GloVe\")\n"
      ],
      "metadata": {
        "id": "ctLGGOoEi58A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 1Ô∏è‚É£ –£—Å—Ç–∞–Ω–æ–≤–∫–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
        "# ------------------------------\n",
        "!pip install gensim matplotlib --quiet\n",
        "\n",
        "# ------------------------------\n",
        "# 2Ô∏è‚É£ –ò–º–ø–æ—Ä—Ç—ã\n",
        "# ------------------------------\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, v_measure_score\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "# ------------------------------\n",
        "# 3Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ LDA\n",
        "# ------------------------------\n",
        "# tokenized_docs = [doc.split() for doc in preprocessed_texts]  # –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É —Ç–µ–±—è\n",
        "num_topics = 10  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ ¬´—Ç–µ–º¬ª –¥–ª—è –ø—Å–µ–≤–¥–æ-—Ä–∞–∑–º–µ—Ç–∫–∏\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics,\n",
        "               passes=5, random_state=42)\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â—É—é —Ç–µ–º—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "lda_labels = []\n",
        "for bow in corpus:\n",
        "    if len(bow) == 0:\n",
        "        lda_labels.append(-1)  # –ø—É—Å—Ç–æ–π –¥–æ–∫—É–º–µ–Ω—Ç\n",
        "    else:\n",
        "        probs = lda.get_document_topics(bow)\n",
        "        top = max(probs, key=lambda x: x[1])\n",
        "        lda_labels.append(top[0])\n",
        "lda_labels = np.array(lda_labels)\n",
        "# –ò—Å–∫–ª—é—á–∞–µ–º –ø—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
        "mask = lda_labels != -1\n",
        "y_true = lda_labels[mask]\n",
        "\n",
        "# ------------------------------\n",
        "# 4Ô∏è‚É£ –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å—á—ë—Ç–∞ –≤–Ω–µ—à–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
        "# ------------------------------\n",
        "def plot_external_metrics(X, y_true, k_range, title=\"\"):\n",
        "    ari_scores, nmi_scores, v_scores = [], [], []\n",
        "\n",
        "    # –ï—Å–ª–∏ –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è, –≤–æ–∑—å–º—ë–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
        "    from scipy.sparse import issparse\n",
        "    if issparse(X):\n",
        "        X_eval = X[:1500].toarray()\n",
        "        y_eval = y_true[:1500]\n",
        "    else:\n",
        "        X_eval = X\n",
        "        y_eval = y_true\n",
        "\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42).fit(X_eval)\n",
        "        labels_pred = kmeans.labels_\n",
        "\n",
        "        ari_scores.append(adjusted_rand_score(y_eval, labels_pred))\n",
        "        nmi_scores.append(normalized_mutual_info_score(y_eval, labels_pred))\n",
        "        v_scores.append(v_measure_score(y_eval, labels_pred))\n",
        "\n",
        "    # –ü–æ—Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏\n",
        "    plt.figure(figsize=(15,4))\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.plot(k_range, ari_scores, marker='o')\n",
        "    plt.title(f\"Adjusted Rand Index ({title})\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(k_range, nmi_scores, marker='o')\n",
        "    plt.title(f\"Normalized Mutual Info ({title})\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot(k_range, v_scores, marker='o')\n",
        "    plt.title(f\"V-Measure ({title})\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------\n",
        "# 5Ô∏è‚É£ –ó–∞–ø—É—Å–∫ –¥–ª—è —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "# ------------------------------\n",
        "k_range = range(2, 16)\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä: Word2Vec –º–∞—Ç—Ä–∏—Ü–∞\n",
        "plot_external_metrics(X_w2v, y_true, k_range, title=\"Word2Vec\")\n",
        "\n",
        "# –ú–æ–∂–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –¥–ª—è X_tfidf, X_ft, X_glove\n",
        "plot_external_metrics(X_tfidf, y_true, k_range, title=\"TF-IDF\")\n",
        "plot_external_metrics(X_ft, y_true, k_range, title=\"FastText\")\n",
        "plot_external_metrics(X_glove, y_true, k_range, title=\"GloVe\")\n"
      ],
      "metadata": {
        "id": "adcHdppgoAly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.5. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**\n",
        "- –î–ª—è TF-IDF: —Ç–æ–ø-10 —Å–ª–æ–≤ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä  \n",
        "- –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –±–ª–∏–∂–∞–π—à–∏–µ —Å–ª–æ–≤–∞ –∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É (—á–µ—Ä–µ–∑ `model.most_similar()`)  \n",
        "- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: UMAP ‚Üí 2D scatter plot —Å —Ü–≤–µ—Ç–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "\n",
        "> üìå **LIME –∏ SHAP** –º–æ–≥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è **—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫ –≤ downstream-–∑–∞–¥–∞—á–µ** (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Ç–µ—Ä ‚Üí –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞). –í —á–∏—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –æ–Ω–∏ **–Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º—ã –Ω–∞–ø—Ä—è–º—É—é**, –Ω–æ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Å–≤–µ–Ω–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤.\n"
      ],
      "metadata": {
        "id": "7Q75XrFhrCVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import umap\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ TF-IDF: —Ç–æ–ø-10 —Å–ª–æ–≤ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä\n",
        "# -----------------------------\n",
        "def tfidf_top_words(X_tfidf, vectorizer, k=10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_tfidf)\n",
        "    labels = kmeans.labels_\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "    print(\"TF-IDF: —Ç–æ–ø-10 —Å–ª–æ–≤ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä\")\n",
        "    for cluster in range(k):\n",
        "        # —Å—Ä–µ–¥–Ω–∏–π –≤–µ—Å —Å–ª–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
        "        cluster_center = kmeans.cluster_centers_[cluster]\n",
        "        top_idx = cluster_center.argsort()[-10:][::-1]\n",
        "        top_words = feature_names[top_idx]\n",
        "        print(f\"Cluster {cluster}: {', '.join(top_words)}\")\n",
        "    return labels\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: –±–ª–∏–∂–∞–π—à–∏–µ —Å–ª–æ–≤–∞ –∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É\n",
        "# -----------------------------\n",
        "def embeddings_top_words(model, X_emb, k=10, topn=10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_emb)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    print(\"\\n–≠–º–±–µ–¥–¥–∏–Ω–≥–∏: –±–ª–∏–∂–∞–π—à–∏–µ —Å–ª–æ–≤–∞ –∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É\")\n",
        "    for cluster in range(k):\n",
        "        centroid = kmeans.cluster_centers_[cluster]\n",
        "        # most_similar –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –≤–µ–∫—Ç–æ—Ä —á–µ—Ä–µ–∑ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫\n",
        "        try:\n",
        "            similar = model.wv.similar_by_vector(centroid, topn=topn)\n",
        "            words = [w for w,_ in similar]\n",
        "            print(f\"Cluster {cluster}: {', '.join(words)}\")\n",
        "        except:\n",
        "            print(f\"Cluster {cluster}: –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–ª–æ–≤–∞\")\n",
        "    return labels\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ UMAP: –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 2D scatter\n",
        "# -----------------------------\n",
        "def plot_umap(X, labels, title=\"UMAP 2D Visualization\"):\n",
        "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
        "    X_2d = reducer.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    scatter = plt.scatter(X_2d[:,0], X_2d[:,1], c=labels, cmap='tab10', s=10)\n",
        "    plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "# -----------------------------\n",
        "# TF-IDF\n",
        "tfidf_labels = tfidf_top_words(X_tfidf, tfidf_vectorizer, k=10)\n",
        "plot_umap(X_tfidf.toarray(), tfidf_labels, title=\"TF-IDF UMAP\")\n",
        "\n",
        "# Word2Vec\n",
        "w2v_labels = embeddings_top_words(w2v_model, X_w2v, k=10)\n",
        "plot_umap(X_w2v, w2v_labels, title=\"Word2Vec UMAP\")\n",
        "\n",
        "# FastText\n",
        "ft_labels = embeddings_top_words(ft_model, X_ft, k=10)\n",
        "plot_umap(X_ft, ft_labels, title=\"FastText UMAP\")\n",
        "\n",
        "# GloVe\n",
        "glove_labels = embeddings_top_words(glove_model, X_glove, k=10)\n",
        "plot_umap(X_glove, glove_labels, title=\"GloVe UMAP\")\n"
      ],
      "metadata": {
        "id": "-icthX6MrDZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **3.6. AutoML –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏?**\n",
        "> ‚ùó **–í–∞–∂–Ω–æ**: **AutoML-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ (Auto-sklearn, TPOT, H2O)** **–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –∑–∞–¥–∞—á–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è** (–≤–∫–ª—é—á–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é) –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.  \n",
        "> Auto-sklearn –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **—Ç–æ–ª—å–∫–æ –∑–∞–¥–∞—á–∏ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π** (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è).  \n",
        "> –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, **AutoML –≤ –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø**."
      ],
      "metadata": {
        "id": "aKV8y5IQtaM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoML –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è, —Ç–∞–∫ –∫–∞–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ–¥—Ö–æ–¥–∏—Ç —Ç–æ–ª—å–∫–æ unsupervised –ø–æ–¥—Ö–æ–¥, –∞ AutoML —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –Ω–∞ supervised –∑–∞–¥–∞—á–∏."
      ],
      "metadata": {
        "id": "9BS5WaFet29Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.7. –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `Streamlit` –∏–ª–∏ `Gradio` –¥–ª—è:\n",
        "  - –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏/–∞–ª–≥–æ—Ä–∏—Ç–º–∞,  \n",
        "  - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤,  \n",
        "  - –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.\n"
      ],
      "metadata": {
        "id": "D_XmY98EvHui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app4.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ session_state\n",
        "X_options = {\n",
        "    \"TF-IDF\": st.session_state.get('X_tfidf'),\n",
        "    \"Word2Vec\": st.session_state.get('X_w2v'),\n",
        "    \"FastText\": st.session_state.get('X_ft'),\n",
        "    \"GloVe\": st.session_state.get('X_glove')\n",
        "}\n",
        "tokenized_docs = st.session_state.get('tokenized_docs', [])\n",
        "\n",
        "st.title(\"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\")\n",
        "\n",
        "# –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\n",
        "X_name = st.selectbox(\"–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\", list(X_options.keys()))\n",
        "n_clusters = st.slider(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\", 2, 20, 10)\n",
        "\n",
        "if st.button(\"–í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é\"):\n",
        "    X = X_options[X_name]\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    st.write(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è {X_name}:\")\n",
        "\n",
        "    # –ü—Ä–æ—Å–º–æ—Ç—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º\n",
        "    cluster_id = st.number_input(\"–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø—Ä–∏–º–µ—Ä–æ–≤\", 0, n_clusters-1, 0)\n",
        "    examples = [doc for doc, label in zip(tokenized_docs, labels) if label == cluster_id][:10]\n",
        "    for ex in examples:\n",
        "        st.write(\" \".join(ex))\n",
        "\n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ PCA –≤–º–µ—Å—Ç–æ UMAP\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X_2d = pca.fit_transform(X)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(X_2d[:,0], X_2d[:,1], c=labels, cmap='tab10', s=10)\n",
        "    plt.title(f\"PCA scatter plot ({X_name})\")\n",
        "    st.pyplot(plt)\n"
      ],
      "metadata": {
        "id": "QY_u5cyQxDXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –û—á–∏—Å—Ç–∫–∞ session_state\n",
        "for key in list(st.session_state.keys()):\n",
        "    del st.session_state[key]\n"
      ],
      "metadata": {
        "id": "HUfJJotf4271"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "st.session_state['X_tfidf'] = X_tfidf[:1500].toarray() if hasattr(X_tfidf, \"toarray\") else X_tfidf[:1500]\n",
        "st.session_state['X_w2v'] = X_w2v[:1500]\n",
        "st.session_state['X_ft'] = X_ft[:1500]\n",
        "st.session_state['X_glove'] = X_glove[:1500]\n",
        "st.session_state['tokenized_docs'] = tokenized_docs[:1500]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vKQ4_YKtxH0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app4.py &>/dev/null &\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "import time\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"33rrvngq55Ym4ttNvRgvHJi6I9z_3uyg5NWWBepyuEtYCYiey\"\n",
        "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "print(\"Public URL –¥–ª—è Streamlit:\", public_url)\n",
        "\n",
        "time.sleep(2)\n",
        "print(\"–ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—É—Å—Ç–∞—è, –ø—Ä–æ–≤–µ—Ä—å –ª–æ–≥: !tail -n 200 /content/nohup.out\")\n"
      ],
      "metadata": {
        "id": "IuT182DPxMez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "Zi0SnHgm4HVx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}